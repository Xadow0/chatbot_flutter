/// Modelos de datos para Ollama Local (LLM ejecut√°ndose en la m√°quina local)

/// Estados posibles del servicio Ollama Local
enum OllamaLocalStatus {
  /// El servicio Ollama no est√° ejecut√°ndose
  stopped,
  
  /// Verificando conexi√≥n con Ollama
  connecting,
  
  /// Conectado y listo para usar
  ready,
  
  /// Ocurri√≥ un error
  error,
}

extension OllamaLocalStatusExtension on OllamaLocalStatus {
  /// Texto descriptivo del estado
  String get displayText {
    switch (this) {
      case OllamaLocalStatus.stopped:
        return 'Detenido';
      case OllamaLocalStatus.connecting:
        return 'Conectando...';
      case OllamaLocalStatus.ready:
        return 'Listo';
      case OllamaLocalStatus.error:
        return 'Error';
    }
  }

  /// Emoji representativo del estado
  String get emoji {
    switch (this) {
      case OllamaLocalStatus.stopped:
        return '‚ö´';
      case OllamaLocalStatus.connecting:
        return 'üü°';
      case OllamaLocalStatus.ready:
        return 'üü¢';
      case OllamaLocalStatus.error:
        return 'üî¥';
    }
  }

  /// Indica si el servicio puede usarse
  bool get isUsable => this == OllamaLocalStatus.ready;
}

/// Resultado de la inicializaci√≥n de Ollama Local
class OllamaLocalInitResult {
  final bool success;
  final String? error;
  final String? modelName;
  final List<String>? availableModels;

  OllamaLocalInitResult({
    required this.success,
    this.error,
    this.modelName,
    this.availableModels,
  });

  /// Mensaje amigable para mostrar al usuario
  String get userMessage {
    if (success) {
      return '‚úÖ Ollama Local conectado correctamente\n'
             'ü§ñ Modelo activo: $modelName\n'
             'üìã Modelos disponibles: ${availableModels?.length ?? 0}';
    } else {
      return '‚ùå Error: ${error ?? "Desconocido"}';
    }
  }
}

/// Informaci√≥n sobre un modelo de Ollama Local
class OllamaLocalModelInfo {
  final String name;
  final String displayName;
  final String description;
  final bool isDownloaded;
  final String size;

  OllamaLocalModelInfo({
    required this.name,
    required this.displayName,
    required this.description,
    required this.isDownloaded,
    required this.size,
  });

  /// Modelos recomendados para Ollama Local
  static List<OllamaLocalModelInfo> get recommendedModels => [
    OllamaLocalModelInfo(
      name: 'phi3',
      displayName: 'Phi-3 Mini',
      description: 'Modelo ligero de Microsoft (3.8B par√°metros)\n'
                   'R√°pido y eficiente para uso local',
      isDownloaded: false,
      size: '2.3 GB',
    ),
    OllamaLocalModelInfo(
      name: 'mistral',
      displayName: 'Mistral 7B',
      description: 'Modelo de alto rendimiento (7B par√°metros)\n'
                   'Excelente balance entre calidad y velocidad',
      isDownloaded: false,
      size: '4.1 GB',
    ),
    OllamaLocalModelInfo(
      name: 'llama3.2',
      displayName: 'Llama 3.2',
      description: 'Modelo de Meta (3B par√°metros)\n'
                   'Muy eficiente para tareas generales',
      isDownloaded: false,
      size: '2.0 GB',
    ),
    OllamaLocalModelInfo(
      name: 'gemma2:2b',
      displayName: 'Gemma 2 2B',
      description: 'Modelo de Google (2B par√°metros)\n'
                   'Ultraligero y r√°pido',
      isDownloaded: false,
      size: '1.6 GB',
    ),
  ];
}

/// Excepci√≥n personalizada para errores de Ollama Local
class OllamaLocalException implements Exception {
  final String message;
  final String? details;

  OllamaLocalException(this.message, {this.details});

  @override
  String toString() => 'OllamaLocalException: $message${details != null ? '\nDetalles: $details' : ''}';

  /// Mensaje amigable para el usuario
  String get userFriendlyMessage {
    switch (message) {
      case 'Ollama no est√° ejecut√°ndose':
        return '‚ùå Ollama no est√° ejecut√°ndose en tu computadora\n\n'
               'üí° Soluci√≥n:\n'
               '1. Abre PowerShell o Terminal\n'
               '2. Ejecuta: ollama serve\n'
               '3. Mant√©n la terminal abierta\n'
               '4. Intenta conectar de nuevo';
      
      case 'Modelo no encontrado':
        return '‚ùå El modelo "$details" no est√° descargado\n\n'
               'üí° Soluci√≥n:\n'
               '1. Abre PowerShell o Terminal\n'
               '2. Ejecuta: ollama pull $details\n'
               '3. Espera a que termine la descarga\n'
               '4. Intenta de nuevo';
      
      case 'Error de conexi√≥n':
        return '‚ùå No se pudo conectar a Ollama\n\n'
               'üí° Soluciones:\n'
               '‚Ä¢ Verifica que Ollama est√© ejecut√°ndose (ollama serve)\n'
               '‚Ä¢ Comprueba que no haya un firewall bloqueando el puerto 11434\n'
               '‚Ä¢ Reinicia Ollama';
      
      case 'Timeout':
        return '‚ùå Ollama tard√≥ demasiado en responder\n\n'
               'üí° Posibles causas:\n'
               '‚Ä¢ El modelo se est√° cargando por primera vez (puede tardar)\n'
               '‚Ä¢ Tu computadora necesita m√°s recursos\n'
               '‚Ä¢ El prompt es muy complejo\n\n'
               'Intenta de nuevo en unos segundos';
      
      default:
        return '‚ùå $message\n\n${details ?? ""}';
    }
  }
}

/// Configuraci√≥n de Ollama Local
class OllamaLocalConfig {
  final String baseUrl;
  final String defaultModel;
  final double temperature;
  final int maxTokens;
  final Duration timeout;

  const OllamaLocalConfig({
    this.baseUrl = 'http://localhost:11434',
    this.defaultModel = 'phi3',
    this.temperature = 0.7,
    this.maxTokens = 2048,
    this.timeout = const Duration(seconds: 60),
  });

  OllamaLocalConfig copyWith({
    String? baseUrl,
    String? defaultModel,
    double? temperature,
    int? maxTokens,
    Duration? timeout,
  }) {
    return OllamaLocalConfig(
      baseUrl: baseUrl ?? this.baseUrl,
      defaultModel: defaultModel ?? this.defaultModel,
      temperature: temperature ?? this.temperature,
      maxTokens: maxTokens ?? this.maxTokens,
      timeout: timeout ?? this.timeout,
    );
  }
}