import 'package:flutter/foundation.dart';
import '../models/message_model.dart';
import '../models/ollama_models.dart';
import '../models/local_llm_models.dart';
import 'gemini_service.dart';
import 'ollama_service.dart';
import 'openai_service.dart';
import 'local_llm_service.dart';

enum AIProvider {
  gemini,
  ollama,
  openai,
  localLLM, // Nuevo: modelo local
}

class AIServiceSelector extends ChangeNotifier {
  final GeminiService _geminiService;
  final OllamaService _ollamaService;
  final OpenAIService _openaiService;
  final LocalLLMService _localLLMService; // Nuevo servicio
  
  AIProvider _currentProvider = AIProvider.gemini;
  String _currentOllamaModel = 'phi3:latest';
  String _currentOpenAIModel = 'gpt-4o-mini';
  List<OllamaModel> _availableModels = [];
  bool _ollamaAvailable = false;
  bool _openaiAvailable = false;
  LocalLLMStatus _localLLMStatus = LocalLLMStatus.stopped; // Nuevo: estado del LLM local
  
  AIServiceSelector({
    required GeminiService geminiService,
    required OllamaService ollamaService,
    required OpenAIService openaiService,
    required LocalLLMService localLLMService, // Nuevo par√°metro
  }) : _geminiService = geminiService,
       _ollamaService = ollamaService,
       _openaiService = openaiService,
       _localLLMService = localLLMService { // Inicializar nuevo servicio
    _initializeServices();
    
    // Escuchar cambios de estado del LLM local
    _localLLMService.addStatusListener(_onLocalLLMStatusChanged);
  }
  
  // Getters
  AIProvider get currentProvider => _currentProvider;
  String get currentOllamaModel => _currentOllamaModel;
  String get currentOpenAIModel => _currentOpenAIModel;
  List<OllamaModel> get availableModels => _availableModels;
  List<String> get availableOpenAIModels => OpenAIService.availableModels;
  bool get ollamaAvailable => _ollamaAvailable;
  bool get openaiAvailable => _openaiAvailable;
  OllamaService get ollamaService => _ollamaService;
  OpenAIService get openaiService => _openaiService;
  ConnectionInfo get connectionInfo => _ollamaService.connectionInfo;
  
  // Nuevos getters para LLM local
  LocalLLMService get localLLMService => _localLLMService;
  LocalLLMStatus get localLLMStatus => _localLLMStatus;
  bool get localLLMAvailable => _localLLMStatus == LocalLLMStatus.ready;
  bool get localLLMLoading => _localLLMStatus == LocalLLMStatus.loading;
  String? get localLLMError => _localLLMService.errorMessage;
  
  // Stream de estado de conexi√≥n
  Stream<ConnectionInfo> get connectionStream => _ollamaService.connectionStream;
  
  // Callback para cambios de estado del LLM local
  void _onLocalLLMStatusChanged(LocalLLMStatus status) {
    debugPrint('üì° [AIServiceSelector] Estado LLM local cambi√≥ a: ${status.displayText}');
    _localLLMStatus = status;
    notifyListeners();
  }
  
  // Inicializar todos los servicios
  Future<void> _initializeServices() async {
    debugPrint('üé¨ [AIServiceSelector] Inicializando servicios de IA...');
    
    // Inicializar Ollama
    await _initializeOllama();
    
    // Inicializar OpenAI
    _initializeOpenAI();
    
    // El LLM local se inicializa bajo demanda, no autom√°ticamente
    
    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados');
    debugPrint('   üìä Gemini: Siempre disponible');
    debugPrint('   üìä Ollama: ${_ollamaAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä OpenAI: ${_openaiAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä LLM Local: ${_localLLMStatus.displayText}');
  }
  
  // Inicializar Ollama
  Future<void> _initializeOllama() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando Ollama...');
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      } else {
        debugPrint('   ‚ö†Ô∏è Ollama no disponible en la inicializaci√≥n');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama: $e');
    }
    notifyListeners();
  }
  
  // Inicializar OpenAI
  void _initializeOpenAI() {
    _openaiAvailable = _openaiService.isAvailable;
    if (_openaiAvailable) {
      debugPrint('‚úÖ [AIServiceSelector] OpenAI disponible');
      debugPrint('   ü§ñ Modelos disponibles: ${OpenAIService.availableModels.join(", ")}');
    } else {
      debugPrint('‚ö†Ô∏è [AIServiceSelector] OpenAI no disponible (API Key no configurada)');
    }
  }
  
  // NUEVO: Inicializar LLM Local
  Future<LocalLLMInitResult> initializeLocalLLM() async {
    debugPrint('üöÄ [AIServiceSelector] Iniciando LLM local...');
    
    final result = await _localLLMService.initializeModel();
    
    if (result.success) {
      debugPrint('‚úÖ [AIServiceSelector] LLM local inicializado correctamente');
    } else {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando LLM local: ${result.error}');
    }
    
    notifyListeners();
    return result;
  }
  
  // NUEVO: Detener LLM Local
  Future<void> stopLocalLLM() async {
    debugPrint('üõë [AIServiceSelector] Deteniendo LLM local...');
    
    // Si el proveedor actual es el LLM local, cambiar a Gemini
    if (_currentProvider == AIProvider.localLLM) {
      debugPrint('   üîÑ Cambiando a Gemini antes de detener');
      await setProvider(AIProvider.gemini);
    }
    
    await _localLLMService.stopModel();
    notifyListeners();
  }
  
  // NUEVO: Reintentar inicializaci√≥n del LLM local
  Future<LocalLLMInitResult> retryLocalLLM() async {
    debugPrint('üîÑ [AIServiceSelector] Reintentando inicializaci√≥n del LLM local...');
    return await _localLLMService.retry();
  }
  
  // Verificar disponibilidad de Ollama
  Future<void> _checkOllamaAvailability() async {
    try {
      debugPrint('üíì [AIServiceSelector] Verificando disponibilidad de Ollama...');
      final health = await _ollamaService.checkHealth();
      _ollamaAvailable = health.success && health.ollamaAvailable;
      debugPrint('   ${_ollamaAvailable ? "‚úÖ" : "‚ùå"} Ollama ${_ollamaAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('   ‚ùå Error en health check: $e');
      _ollamaAvailable = false;
    }
  }
  
  // Cargar modelos disponibles
  Future<void> _loadAvailableModels() async {
    try {
      debugPrint('üìã [AIServiceSelector] Cargando modelos disponibles...');
      _availableModels = await _ollamaService.getModels();
      
      // Si el modelo actual no est√° disponible, seleccionar el primero
      if (_availableModels.isNotEmpty && 
          !_availableModels.any((m) => m.name == _currentOllamaModel)) {
        final oldModel = _currentOllamaModel;
        _currentOllamaModel = _availableModels.first.name;
        debugPrint('   ‚ö†Ô∏è Modelo $oldModel no encontrado, usando ${_availableModels.first.name}');
      } else {
        debugPrint('   ‚úÖ Modelo actual $_currentOllamaModel est√° disponible');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error cargando modelos: $e');
      _availableModels = [];
    }
  }
  
  // Cambiar proveedor
  Future<void> setProvider(AIProvider provider) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando proveedor a: $provider');
    
    // Validaciones seg√∫n el proveedor
    if (provider == AIProvider.ollama) {
      await _checkOllamaAvailability();
      if (!_ollamaAvailable) {
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a Ollama: no disponible');
        throw Exception('Ollama no est√° disponible. Verifica que el servidor est√© accesible.');
      }
    } else if (provider == AIProvider.openai) {
      if (!_openaiAvailable) {
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a OpenAI: no disponible');
        throw Exception('OpenAI no est√° disponible. Configura OPENAI_API_KEY en el archivo .env');
      }
    } else if (provider == AIProvider.localLLM) {
      // NUEVO: Validaci√≥n para LLM local
      if (_localLLMStatus != LocalLLMStatus.ready) {
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a LLM local: estado=${_localLLMStatus.displayText}');
        throw Exception('El modelo local no est√° listo. Estado: ${_localLLMStatus.displayText}');
      }
    }
    
    _currentProvider = provider;
    debugPrint('‚úÖ [AIServiceSelector] Proveedor cambiado a: $provider');
    notifyListeners();
  }
  
  // Cambiar modelo de Ollama
  Future<void> setOllamaModel(String model) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama a: $model');
    
    if (await _ollamaService.isModelAvailable(model)) {
      _currentOllamaModel = model;
      debugPrint('‚úÖ [AIServiceSelector] Modelo Ollama cambiado a: $model');
      notifyListeners();
    } else {
      debugPrint('‚ùå [AIServiceSelector] Modelo $model no est√° disponible');
      throw Exception('Modelo $model no est√° disponible');
    }
  }
  
  // Cambiar modelo de OpenAI
  void setOpenAIModel(String model) {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo OpenAI a: $model');
    
    if (_openaiService.isModelAvailable(model)) {
      _currentOpenAIModel = model;
      debugPrint('‚úÖ [AIServiceSelector] Modelo OpenAI cambiado a: $model');
      notifyListeners();
    } else {
      debugPrint('‚ùå [AIServiceSelector] Modelo OpenAI $model no est√° disponible');
      throw Exception('Modelo OpenAI $model no est√° disponible');
    }
  }
  
  // Refrescar estado de Ollama
  Future<void> refreshOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando Ollama...');
    await _ollamaService.reconnect();
    await _initializeOllama();
  }
  
  // Refrescar todos los servicios
  Future<void> refreshAllServices() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando todos los servicios...');
    await _initializeServices();
  }
  
  // Enviar mensaje usando el proveedor actual
  Future<String> sendMessage(String message, {List<Message>? history}) async {
    debugPrint('\nüöÄ [AIServiceSelector] === ENVIANDO MENSAJE ===');
    debugPrint('   ü§ñ Proveedor: $_currentProvider');
    debugPrint('   üí¨ Mensaje: ${message.length > 50 ? "${message.substring(0, 50)}..." : message}');
    debugPrint('   üìö Historial: ${history?.length ?? 0} mensajes');
    
    try {
      switch (_currentProvider) {
        case AIProvider.gemini:
          debugPrint('   üü¶ Enviando a Gemini...');
          return await _sendToGemini(message, history);
        case AIProvider.ollama:
          debugPrint('   üü™ Enviando a Ollama...');
          return await _sendToOllama(message, history);
        case AIProvider.openai:
          debugPrint('   üü© Enviando a OpenAI...');
          return await _sendToOpenAI(message, history);
        case AIProvider.localLLM:
          // NUEVO: Enviar al LLM local
          debugPrint('   üüß Enviando a LLM Local...');
          return await _sendToLocalLLM(message, history);
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error enviando mensaje: $e');
      debugPrint('üî¥ [AIServiceSelector] === ENV√çO FALLIDO ===\n');
      rethrow;
    }
  }
  
  // Enviar a Gemini
  Future<String> _sendToGemini(String message, List<Message>? history) async {
    try {
      final response = await _geminiService.generateContent(message);
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Gemini recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Gemini: $e');
      throw Exception('Error con Gemini: $e');
    }
  }
  
  // Enviar a Ollama
  Future<String> _sendToOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando disponibilidad del modelo $_currentOllamaModel...');
      
      // Verificar que el modelo est√© disponible
      final isAvailable = await _ollamaService.isModelAvailable(_currentOllamaModel);
      if (!isAvailable) {
        debugPrint('   ‚ùå Modelo $_currentOllamaModel no disponible');
        throw Exception('Modelo $_currentOllamaModel no disponible');
      }
      
      debugPrint('   ‚úì Modelo disponible');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        // Usar chat con historial
        final chatMessages = _convertHistoryToChatMessages(history, message);
        response = await _ollamaService.chatWithHistory(
          model: _currentOllamaModel,
          messages: chatMessages,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        // Usar generaci√≥n simple
        response = await _ollamaService.generateResponse(
          model: _currentOllamaModel,
          prompt: message,
          systemPrompt: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting.',
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama: $e');
      
      // Proporcionar informaci√≥n de diagn√≥stico
      if (e.toString().contains('connection') || e.toString().contains('Socket')) {
        debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
        debugPrint('   1. Verifica Tailscale en ambos dispositivos');
        debugPrint('   2. Confirma que el servidor est√© corriendo');
        debugPrint('   3. Prueba: curl ${_ollamaService.baseUrl}/api/health');
      } else if (e.toString().contains('Timeout')) {
        debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
        debugPrint('   1. El modelo puede estar carg√°ndose por primera vez');
        debugPrint('   2. La consulta puede ser muy compleja');
        debugPrint('   3. Intenta con un prompt m√°s corto');
      }
      
      throw Exception('Error con Ollama: $e');
    }
  }
  
  // Enviar a OpenAI
  Future<String> _sendToOpenAI(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Usando modelo: $_currentOpenAIModel');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        // Convertir historial al formato de OpenAI
        final messages = <Map<String, String>>[];
        
        // Limitar historial a √∫ltimos 10 mensajes
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          messages.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        // Agregar mensaje actual
        messages.add({
          'role': 'user',
          'content': message,
        });
        
        response = await _openaiService.chatWithHistory(
          messages: messages,
          model: _currentOpenAIModel,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _openaiService.generateContent(
          message,
          model: _currentOpenAIModel,
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de OpenAI recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con OpenAI: $e');
      
      // Proporcionar informaci√≥n de diagn√≥stico
      if (e.toString().contains('401') || e.toString().contains('inv√°lida')) {
        debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
        debugPrint('   1. Verifica que OPENAI_API_KEY sea correcta en .env');
        debugPrint('   2. La API key debe empezar con "sk-"');
        debugPrint('   3. Verifica que la key tenga cr√©ditos disponibles');
      } else if (e.toString().contains('429')) {
        debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
        debugPrint('   1. Has excedido el l√≠mite de solicitudes');
        debugPrint('   2. Espera unos segundos antes de reintentar');
        debugPrint('   3. Considera usar un modelo m√°s econ√≥mico (gpt-3.5-turbo)');
      } else if (e.toString().contains('conexi√≥n')) {
        debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
        debugPrint('   1. Verifica tu conexi√≥n a internet');
        debugPrint('   2. OpenAI puede estar temporalmente no disponible');
      }
      
      throw Exception('Error con OpenAI: $e');
    }
  }
  
  // NUEVO: Enviar al LLM Local
  Future<String> _sendToLocalLLM(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando estado del LLM local...');
      
      if (_localLLMStatus != LocalLLMStatus.ready) {
        debugPrint('   ‚ùå LLM local no est√° listo: ${_localLLMStatus.displayText}');
        throw Exception('El modelo local no est√° listo');
      }
      
      debugPrint('   ‚úì LLM local disponible');
      debugPrint('   üí≠ Generando respuesta localmente...');
      
      // Por ahora, solo enviamos el mensaje actual
      // En el futuro, se puede agregar soporte para historial
      final response = await _localLLMService.generateContent(message);
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta del LLM local recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con LLM local: $e');
      
      // Proporcionar informaci√≥n de diagn√≥stico
      debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
      debugPrint('   1. El modelo puede necesitar reiniciarse');
      debugPrint('   2. Verifica que haya suficiente RAM disponible');
      debugPrint('   3. Intenta con un prompt m√°s corto');
      
      throw Exception('Error con LLM local: $e');
    }
  }
  
  // Convertir historial a formato de chat para Ollama
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];
    
    // Agregar historial (√∫ltimos 10 mensajes para no sobrecargar)
    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;
    
    debugPrint('   üìö Convirtiendo historial: ${recentHistory.length} mensajes recientes');

    for (final msg in recentHistory) {
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text,
      ));
    }
    
    // Agregar mensaje actual
    messages.add(ChatMessage(
      role: 'user',
      content: newMessage,
    ));
    
    debugPrint('   ‚úì Total de mensajes para chat: ${messages.length}');
    
    return messages;
  }
  
  @override
  void dispose() {
    debugPrint('üî¥ [AIServiceSelector] Disposing...');
    _localLLMService.removeStatusListener(_onLocalLLMStatusChanged);
    _localLLMService.dispose();
    _ollamaService.dispose();
    super.dispose();
  }
}