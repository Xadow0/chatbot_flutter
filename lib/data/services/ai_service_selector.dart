import 'package:flutter/foundation.dart';
import '../models/remote_ollama_models.dart';
import '../models/local_ollama_models.dart';
import 'gemini_service.dart';
import 'ollama_service.dart';
import 'openai_service.dart';
import 'local_ollama_service.dart';
import 'dart:async';
import 'ai_service_adapters.dart';
import '../../domain/usecases/command_processor.dart';

enum AIProvider {
  gemini,
  ollama,
  openai,
  localOllama,
}

class AIServiceSelector extends ChangeNotifier {
  final GeminiService _geminiService;
  final OllamaService _ollamaService;
  final OpenAIService _openaiService;
  final OllamaManagedService _localOllamaService;

  AIProvider _currentProvider = AIProvider.gemini;
  String _currentOllamaModel = 'phi3:latest';
  String _currentOpenAIModel = 'gpt-4o-mini';
  List<OllamaModel> _availableModels = [];
  bool _ollamaAvailable = false;
  bool get isLocalOllamaSupported => _localOllamaService.isPlatformSupported;

  bool _openaiAvailable = false;

  StreamSubscription? _ollamaConnectionSubscription;

  LocalOllamaStatus _localOllamaStatus = LocalOllamaStatus.notInitialized;

  AIServiceSelector({
    required GeminiService geminiService,
    required OllamaService ollamaService,
    required OpenAIService openaiService,
    required OllamaManagedService localOllamaService,
  })  : _geminiService = geminiService,
        _ollamaService = ollamaService,
        _openaiService = openaiService,
        _localOllamaService = localOllamaService {
    _ollamaConnectionSubscription = _ollamaService.connectionStream.listen(_onOllamaConnectionChanged);

    _localOllamaService.addStatusListener(_onLocalOllamaStatusChanged);

    _initializeOpenAI();

    _onOllamaConnectionChanged(_ollamaService.connectionInfo);

    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados y escuchando cambios...');
  }

  AIProvider get currentProvider => _currentProvider;
  String get currentOllamaModel => _currentOllamaModel;
  String get currentOpenAIModel => _currentOpenAIModel;
  List<OllamaModel> get availableModels => _availableModels;
  List<String> get availableOpenAIModels => OpenAIService.availableModels;
  bool get ollamaAvailable => _ollamaAvailable;

  bool get openaiAvailable => _openaiAvailable;

  OllamaService get ollamaService => _ollamaService;
  OpenAIService get openaiService => _openaiService;
  ConnectionInfo get connectionInfo => _ollamaService.connectionInfo;

  GeminiService get geminiService => _geminiService;
  OllamaManagedService get localOllamaService => _localOllamaService;
  LocalOllamaStatus get localOllamaStatus => _localOllamaStatus;
  bool get localOllamaAvailable => _localOllamaStatus == LocalOllamaStatus.ready;
  bool get localOllamaLoading => _localOllamaStatus.isProcessing;
  String? get localOllamaError => _localOllamaService.errorMessage;

  Stream<ConnectionInfo> get connectionStream => _ollamaService.connectionStream;

  void _onLocalOllamaStatusChanged(LocalOllamaStatus status) {
    debugPrint('üì° [AIServiceSelector] Estado Ollama Local cambi√≥ a: ${status.displayText}');
    _localOllamaStatus = status;
    notifyListeners();
  }

  Future<void> refreshOpenAIAvailability() async {
    try {
      debugPrint('üîÑ [AIServiceSelector] Verificando disponibilidad de OpenAI...');
      _openaiAvailable = await _openaiService.isAvailable();
      debugPrint('   ${_openaiAvailable ? "‚úÖ" : "‚ùå"} OpenAI ${_openaiAvailable ? "disponible" : "no disponible"}');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error verificando OpenAI: $e');
      _openaiAvailable = false;
      notifyListeners();
    }
  }

  Future<void> refreshOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando Ollama...');
    try {
      await _ollamaService.reconnect();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error refrescando Ollama: $e');
    }
  }

  Future<void> _onOllamaConnectionChanged(ConnectionInfo info) async {
    debugPrint('üì° [AIServiceSelector] Estado Ollama Remoto cambi√≥ a: ${info.status}');

    if (info.status == ConnectionStatus.connected) {
      final wasAvailable = _ollamaAvailable;
      _ollamaAvailable = true;

      if (!wasAvailable) {
        debugPrint('   -> Conexi√≥n establecida. Cargando modelos...');
        await _loadAvailableModels();
      }
    } else {
      if (_ollamaAvailable) {
        debugPrint('   -> Conexi√≥n perdida. Vaciando modelos.');
      }
      _ollamaAvailable = false;
      _availableModels = [];
    }

    notifyListeners();
  }

  Future<void> _initializeOllama() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando Ollama remoto...');
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      } else {
        debugPrint('   ‚ö†Ô∏è Ollama remoto no disponible en la inicializaci√≥n');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama: $e');
    }
    notifyListeners();
  }

  Future<void> _initializeOpenAI() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando OpenAI...');
      _openaiAvailable = await _openaiService.isAvailable();
      debugPrint('   ${_openaiAvailable ? "‚úÖ" : "‚ö†Ô∏è"} OpenAI ${_openaiAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando OpenAI: $e');
      _openaiAvailable = false;
    }
  }

  Future<LocalOllamaInitResult> initializeLocalOllama() async {
    debugPrint('üöÄ [AIServiceSelector] Iniciando Ollama Local...');

    final result = await _localOllamaService.initialize();

    if (result.success) {
      debugPrint('‚úÖ [AIServiceSelector] Ollama Local inicializado correctamente');
      debugPrint('   ü§ñ Modelo activo: ${result.modelName}');
      debugPrint('   üìã Modelos disponibles: ${result.availableModels?.join(", ")}');
    } else {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama Local: ${result.error}');
    }

    notifyListeners();
    return result;
  }

  Future<void> stopLocalOllama() async {
    debugPrint('üõë [AIServiceSelector] Deteniendo Ollama Local...');

    if (_currentProvider == AIProvider.localOllama) {
      debugPrint('   üîÑ Cambiando a Gemini antes de detener');
      await setProvider(AIProvider.gemini);
    }

    await _localOllamaService.stop();
    notifyListeners();
  }

  Future<LocalOllamaInitResult> retryLocalOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Reintentando inicializaci√≥n de Ollama Local...');
    return await _localOllamaService.retry();
  }

  Future<void> _checkOllamaAvailability() async {
    try {
      debugPrint('üíì [AIServiceSelector] Verificando disponibilidad de Ollama remoto...');
      final health = await _ollamaService.checkHealth();
      _ollamaAvailable = health.success && health.ollamaAvailable;
      debugPrint('   ${_ollamaAvailable ? "‚úÖ" : "‚ùå"} Ollama remoto ${_ollamaAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('   ‚ùå Error en health check: $e');
      _ollamaAvailable = false;
    }
  }

  Future<void> _loadAvailableModels() async {
    try {
      debugPrint('üìã [AIServiceSelector] Cargando modelos de Ollama remoto...');
      _availableModels = await _ollamaService.getModels();

      if (_availableModels.isNotEmpty) {
        final modelExists = _availableModels.any((m) => m.name == _currentOllamaModel);
        if (modelExists) {
          debugPrint('   ‚úÖ Modelo actual $_currentOllamaModel est√° disponible');
        } else {
          final oldModel = _currentOllamaModel;
          _currentOllamaModel = _availableModels.first.name;
          debugPrint('   ‚ö†Ô∏è Modelo $oldModel no encontrado, usando ${_availableModels.first.name}');
        }
      } else {
        debugPrint('   ‚ùå No se encontraron modelos en el servidor.');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error cargando modelos: $e');
      _availableModels = [];
    }
  }

  Future<void> setProvider(AIProvider provider) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando proveedor a: $provider');

    if (provider == AIProvider.ollama && !_ollamaAvailable) {
      debugPrint('   ‚ö†Ô∏è Ollama remoto no est√° disponible');
      throw Exception('Ollama remoto no est√° disponible');
    }

    if (provider == AIProvider.openai && !_openaiAvailable) {
      debugPrint('   ‚ö†Ô∏è OpenAI no est√° disponible');
      throw Exception('OpenAI no est√° disponible. Configure su API Key en Ajustes');
    }

    if (provider == AIProvider.localOllama) {
      if (!isLocalOllamaSupported) {
        debugPrint('   ‚ö†Ô∏è Ollama Local no soportado en esta plataforma');
        throw Exception('Ollama Local no est√° disponible en este dispositivo.');
      }
      if (!localOllamaAvailable) {
        debugPrint('   ‚ö†Ô∏è Ollama Local no est√° listo');
        throw Exception('Ollama Local no est√° listo. Inicial√≠zalo primero.');
      }
    }

    _currentProvider = provider;
    notifyListeners();
    debugPrint('   ‚úÖ Proveedor cambiado a $provider');
  }

  Future<void> setOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama a: $modelName');

    if (!_availableModels.any((m) => m.name == modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no est√° disponible');
      throw Exception('Modelo $modelName no est√° disponible');
    }

    _currentOllamaModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo cambiado a $modelName');
  }

  Future<void> setOpenAIModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo OpenAI a: $modelName');

    if (!OpenAIService.availableModels.contains(modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no est√° disponible');
      throw Exception('Modelo $modelName no est√° disponible');
    }

    _currentOpenAIModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo OpenAI cambiado a $modelName');
  }

  Future<bool> setLocalOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama Local a: $modelName');

    final success = await _localOllamaService.changeModel(modelName);

    if (success) {
      debugPrint('   ‚úÖ Modelo Ollama Local cambiado a $modelName');
    } else {
      debugPrint('   ‚ùå Error cambiando modelo Ollama Local');
    }

    return success;
  }

  @override
  void dispose() {
    debugPrint('üî¥ [AIServiceSelector] Disposing...');
    _ollamaConnectionSubscription?.cancel();
    _localOllamaService.removeStatusListener(_onLocalOllamaStatusChanged);
    _localOllamaService.dispose();
    _ollamaService.dispose();
    super.dispose();
  }

  AIServiceBase getCurrentAdapter() {
    debugPrint('üîå [AIServiceSelector] Obteniendo adaptador para: $_currentProvider');

    switch (_currentProvider) {
      case AIProvider.gemini:
        return GeminiServiceAdapter(_geminiService);
      case AIProvider.openai:
        return OpenAIServiceAdapter(_openaiService);
      case AIProvider.ollama:
        return OllamaServiceAdapter(_ollamaService, _currentOllamaModel);
      case AIProvider.localOllama:
        return LocalOllamaServiceAdapter(_localOllamaService);
    }
  }
}