import 'package:flutter/foundation.dart';
import '../models/message_model.dart';
import '../models/remote_ollama_models.dart';
import '../models/local_ollama_models.dart';
import 'gemini_service.dart';
import 'ollama_service.dart';
import 'openai_service.dart';
import 'local_ollama_service.dart';
import 'dart:async';
import 'ai_service_adapters.dart';
import '../../domain/usecases/command_processor.dart';

enum AIProvider {
  gemini,
  ollama,
  openai,
  localOllama,
}

class AIServiceSelector extends ChangeNotifier {
  final GeminiService _geminiService;
  final OllamaService _ollamaService;
  final OpenAIService _openaiService;
  final OllamaManagedService _localOllamaService;
  
  AIProvider _currentProvider = AIProvider.gemini;
  String _currentOllamaModel = 'phi3:latest';
  String _currentOpenAIModel = 'gpt-4o-mini';
  List<OllamaModel> _availableModels = [];
  bool _ollamaAvailable = false;
  
  // üîê NUEVO: Cache para disponibilidad de OpenAI
  bool _openaiAvailable = false;
  
  StreamSubscription? _ollamaConnectionSubscription;

  LocalOllamaStatus _localOllamaStatus = LocalOllamaStatus.notInitialized;
  
  AIServiceSelector({
    required GeminiService geminiService,
    required OllamaService ollamaService,
    required OpenAIService openaiService,
    required OllamaManagedService localOllamaService,
  }) : _geminiService = geminiService,
       _ollamaService = ollamaService,
       _openaiService = openaiService,
       _localOllamaService = localOllamaService {
    _ollamaConnectionSubscription = 
        _ollamaService.connectionStream.listen(_onOllamaConnectionChanged);
    
    // 2. Escuchar el estado de Ollama Local
    _localOllamaService.addStatusListener(_onLocalOllamaStatusChanged);
    
    // 3. Inicializar OpenAI (esto es de una sola vez)
    _initializeOpenAI();
    
    // 4. Comprobar el estado inicial de Ollama Remoto (por si ya estaba conectado)
    _onOllamaConnectionChanged(_ollamaService.connectionInfo);
    
    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados y escuchando cambios...');
  }
  
  // Getters
  AIProvider get currentProvider => _currentProvider;
  String get currentOllamaModel => _currentOllamaModel;
  String get currentOpenAIModel => _currentOpenAIModel;
  List<OllamaModel> get availableModels => _availableModels;
  List<String> get availableOpenAIModels => OpenAIService.availableModels;
  bool get ollamaAvailable => _ollamaAvailable;
  
  // Devuelve el valor cacheado (bool) en lugar de Future<bool>
  bool get openaiAvailable => _openaiAvailable;
  
  OllamaService get ollamaService => _ollamaService;
  OpenAIService get openaiService => _openaiService;
  ConnectionInfo get connectionInfo => _ollamaService.connectionInfo;
  
  // Getters para Ollama Local
  GeminiService get geminiService => _geminiService;
  OllamaManagedService get localOllamaService => _localOllamaService;
  LocalOllamaStatus get localOllamaStatus => _localOllamaStatus;
  bool get localOllamaAvailable => _localOllamaStatus == LocalOllamaStatus.ready;
  bool get localOllamaLoading => _localOllamaStatus.isProcessing;
  String? get localOllamaError => _localOllamaService.errorMessage;
  
  Stream<ConnectionInfo> get connectionStream => _ollamaService.connectionStream;
  
  void _onLocalOllamaStatusChanged(LocalOllamaStatus status) {
    debugPrint('üì° [AIServiceSelector] Estado Ollama Local cambi√≥ a: ${status.displayText}');
    _localOllamaStatus = status;
    notifyListeners();
  }
  
  // M√©todo para refrescar la disponibilidad de OpenAI
  Future<void> refreshOpenAIAvailability() async {
    try {
      debugPrint('üîÑ [AIServiceSelector] Verificando disponibilidad de OpenAI...');
      _openaiAvailable = await _openaiService.isAvailable();
      debugPrint('   ${_openaiAvailable ? "‚úÖ" : "‚ùå"} OpenAI ${_openaiAvailable ? "disponible" : "no disponible"}');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error verificando OpenAI: $e');
      _openaiAvailable = false;
      notifyListeners();
    }
  }
  
  Future<void> refreshOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando Ollama...');
    try {
      // Esto har√° que OllamaService vuelva a comprobar su conexi√≥n.
      // Si el estado cambia, disparar√° el stream,
      // lo que activar√° nuestro listener _onOllamaConnectionChanged.
      await _ollamaService.reconnect();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error refrescando Ollama: $e');
    }
    // No es necesario hacer nada m√°s, el listener se encarga.
  }
  
  Future<void> _initializeServices() async {
    debugPrint('üé¨ [AIServiceSelector] Inicializando servicios de IA...');
    
    await _initializeOllama();
    
    // Inicializar disponibilidad de OpenAI
    await _initializeOpenAI();
    
    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados');
    debugPrint('   üìä Gemini: Siempre disponible');
    debugPrint('   üìä Ollama (remoto): ${_ollamaAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä OpenAI: ${_openaiAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä Ollama Local: ${_localOllamaStatus.displayText}');
  }

  // A√ëADIR ESTE M√âTODO NUEVO
  Future<void> _onOllamaConnectionChanged(ConnectionInfo info) async {
    debugPrint('üì° [AIServiceSelector] Estado Ollama Remoto cambi√≥ a: ${info.status}');
    
    if (info.status == ConnectionStatus.connected) {
      final wasAvailable = _ollamaAvailable;
      _ollamaAvailable = true;
      
      // Solo cargar modelos si es la primera vez que se conecta
      // o si estaba previamente desconectado
      if (!wasAvailable) {
        debugPrint('   -> Conexi√≥n establecida. Cargando modelos...');
        await _loadAvailableModels(); // Carga los modelos
      }
    } else {
      // Si se desconecta o hay error
      if (_ollamaAvailable) {
        debugPrint('   -> Conexi√≥n perdida. Vaciando modelos.');
      }
      _ollamaAvailable = false;
      _availableModels = []; // Limpia los modelos si no hay conexi√≥n
    }
    
    // Notifica al ChatProvider sobre el cambio
    notifyListeners();
  }
  
  Future<void> _initializeOllama() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando Ollama remoto...');
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      } else {
        debugPrint('   ‚ö†Ô∏è Ollama remoto no disponible en la inicializaci√≥n');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama: $e');
    }
    notifyListeners();
  }
  
  // Inicializar disponibilidad de OpenAI
  Future<void> _initializeOpenAI() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando OpenAI...');
      _openaiAvailable = await _openaiService.isAvailable();
      debugPrint('   ${_openaiAvailable ? "‚úÖ" : "‚ö†Ô∏è"} OpenAI ${_openaiAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando OpenAI: $e');
      _openaiAvailable = false;
    }
  }
  
  Future<LocalOllamaInitResult> initializeLocalOllama() async {
    debugPrint('üöÄ [AIServiceSelector] Iniciando Ollama Local...');
    
    final result = await _localOllamaService.initialize();
    
    if (result.success) {
      debugPrint('‚úÖ [AIServiceSelector] Ollama Local inicializado correctamente');
      debugPrint('   ü§ñ Modelo activo: ${result.modelName}');
      debugPrint('   üìã Modelos disponibles: ${result.availableModels?.join(", ")}');
    } else {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama Local: ${result.error}');
    }
    
    notifyListeners();
    return result;
  }
  
  Future<void> stopLocalOllama() async {
    debugPrint('üõë [AIServiceSelector] Deteniendo Ollama Local...');
    
    if (_currentProvider == AIProvider.localOllama) {
      debugPrint('   üîÑ Cambiando a Gemini antes de detener');
      await setProvider(AIProvider.gemini);
    }
    
    await _localOllamaService.stop();
    notifyListeners();
  }
  
  Future<LocalOllamaInitResult> retryLocalOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Reintentando inicializaci√≥n de Ollama Local...');
    return await _localOllamaService.retry();
  }
  
  Future<void> _checkOllamaAvailability() async {
    try {
      debugPrint('üíì [AIServiceSelector] Verificando disponibilidad de Ollama remoto...');
      final health = await _ollamaService.checkHealth();
      _ollamaAvailable = health.success && health.ollamaAvailable;
      debugPrint('   ${_ollamaAvailable ? "‚úÖ" : "‚ùå"} Ollama remoto ${_ollamaAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('   ‚ùå Error en health check: $e');
      _ollamaAvailable = false;
    }
  }
  
  Future<void> _loadAvailableModels() async {
    try {
      debugPrint('üìã [AIServiceSelector] Cargando modelos de Ollama remoto...');
      _availableModels = await _ollamaService.getModels();
      
      if (_availableModels.isNotEmpty) {
        final modelExists = _availableModels.any((m) => m.name == _currentOllamaModel);
        if (modelExists) {
          debugPrint('   ‚úÖ Modelo actual $_currentOllamaModel est√° disponible');
        } else {
          final oldModel = _currentOllamaModel;
          _currentOllamaModel = _availableModels.first.name;
          debugPrint('   ‚ö†Ô∏è Modelo $oldModel no encontrado, usando ${_availableModels.first.name}');
        }
      } else {
        // Si no hay modelos, no podemos decir que el modelo actual est√° disponible
        debugPrint('   ‚ùå No se encontraron modelos en el servidor.');
      }
      
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error cargando modelos: $e');
      _availableModels = [];
    }
  }
  
  Future<void> setProvider(AIProvider provider) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando proveedor a: $provider');
    
    if (provider == AIProvider.ollama && !_ollamaAvailable) {
      debugPrint('   ‚ö†Ô∏è Ollama remoto no est√° disponible');
      throw Exception('Ollama remoto no est√° disponible');
    }
    
    // Ahora usa la variable cacheada
    if (provider == AIProvider.openai && !_openaiAvailable) {
      debugPrint('   ‚ö†Ô∏è OpenAI no est√° disponible');
      throw Exception('OpenAI no est√° disponible. Configure su API Key en Ajustes');
    }
    
    if (provider == AIProvider.localOllama && !localOllamaAvailable) {
      debugPrint('   ‚ö†Ô∏è Ollama Local no est√° listo');
      throw Exception('Ollama Local no est√° listo. Inicial√≠zalo primero.');
    }
    
    _currentProvider = provider;
    notifyListeners();
    debugPrint('   ‚úÖ Proveedor cambiado a $provider');
  }
  
  Future<void> setOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama a: $modelName');
    
    if (!_availableModels.any((m) => m.name == modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no est√° disponible');
      throw Exception('Modelo $modelName no est√° disponible');
    }
    
    _currentOllamaModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo cambiado a $modelName');
  }
  
  Future<void> setOpenAIModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo OpenAI a: $modelName');
    
    if (!OpenAIService.availableModels.contains(modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no est√° disponible');
      throw Exception('Modelo $modelName no est√° disponible');
    }
    
    _currentOpenAIModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo OpenAI cambiado a $modelName');
  }
  
  Future<bool> setLocalOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama Local a: $modelName');
    
    final success = await _localOllamaService.changeModel(modelName);
    
    if (success) {
      debugPrint('   ‚úÖ Modelo Ollama Local cambiado a $modelName');
    } else {
      debugPrint('   ‚ùå Error cambiando modelo Ollama Local');
    }
    
    // El listener _onLocalOllamaStatusChanged se activar√°
    // y notificar√° a los listeners (ChatProvider)
    return success;
  }
  
  Future<String> sendMessage(String message, {List<Message>? history}) async {
    debugPrint('üì§ [AIServiceSelector] === ENVIANDO MENSAJE ===');
    debugPrint('   üéØ Proveedor: $_currentProvider');
    debugPrint('   üí¨ Mensaje: ${message.length > 50 ? "${message.substring(0, 50)}..." : message}');
    debugPrint('   üìö Historial: ${history?.length ?? 0} mensajes');
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        return await _sendToGemini(message, history);
      case AIProvider.ollama:
        return await _sendToOllama(message, history);
      case AIProvider.openai:
        return await _sendToOpenAI(message, history);
      case AIProvider.localOllama:
        return await _sendToLocalOllama(message, history);
    }
  }
  
  Future<String> _sendToGemini(String message, List<Message>? history) async {
    try {
      debugPrint('   üíé Usando Gemini...');
      final response = await _geminiService.generateContent(message);
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Gemini recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Gemini: $e');
      throw Exception('Error con Gemini: $e');
    }
  }
  
  Future<String> _sendToOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando disponibilidad del modelo $_currentOllamaModel...');
      
      final isAvailable = await _ollamaService.isModelAvailable(_currentOllamaModel);
      if (!isAvailable) {
        debugPrint('   ‚ùå Modelo $_currentOllamaModel no disponible');
        throw Exception('Modelo $_currentOllamaModel no disponible');
      }
      
      debugPrint('   ‚úì Modelo disponible');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        final chatMessages = _convertHistoryToChatMessages(history, message);
        response = await _ollamaService.chatWithHistory(
          model: _currentOllamaModel,
          messages: chatMessages,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _ollamaService.generateResponse(
          model: _currentOllamaModel,
          prompt: message,
          systemPrompt: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting.',
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama: $e');
      throw Exception('Error con Ollama: $e');
    }
  }
  
  Future<String> _sendToOpenAI(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Usando modelo: $_currentOpenAIModel');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        final messages = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          messages.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        messages.add({
          'role': 'user',
          'content': message,
        });
        
        response = await _openaiService.chatWithHistory(
          messages: messages,
          model: _currentOpenAIModel,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _openaiService.generateContent(
          message,
          model: _currentOpenAIModel,
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de OpenAI recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con OpenAI: $e');
      throw Exception('Error con OpenAI: $e');
    }
  }
  
  Future<String> _sendToLocalOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando estado de Ollama Local...');
      
      if (_localOllamaStatus != LocalOllamaStatus.ready) {
        debugPrint('   ‚ùå Ollama Local no est√° listo: ${_localOllamaStatus.displayText}');
        throw Exception('Ollama Local no est√° listo');
      }
      
      debugPrint('   ‚úì Ollama Local disponible');
      debugPrint('   üí≠ Generando respuesta localmente...');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        final chatHistory = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          chatHistory.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        response = await _localOllamaService.chatWithHistory(
          prompt: message,
          history: chatHistory,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _localOllamaService.generateContent(message);
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama Local recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama Local: $e');
      throw Exception('Error con Ollama Local: $e');
    }
  }
    
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];
    
    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;
    
    debugPrint('   üìö Convirtiendo historial: ${recentHistory.length} mensajes recientes');

    for (final msg in recentHistory) {
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text,
      ));
    }
    
    messages.add(ChatMessage(
      role: 'user',
      content: newMessage,
    ));
    
    debugPrint('   ‚úì Total de mensajes para chat: ${messages.length}');
    
    return messages;
  }
  
  @override
  void dispose() {
    debugPrint('üî¥ [AIServiceSelector] Disposing...');
    _ollamaConnectionSubscription?.cancel();
    _localOllamaService.removeStatusListener(_onLocalOllamaStatusChanged);
    _localOllamaService.dispose();
    _ollamaService.dispose();
    super.dispose();
  }

  AIServiceBase getCurrentAdapter() {
    debugPrint('üîå [AIServiceSelector] Obteniendo adaptador para: $_currentProvider');
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        return GeminiServiceAdapter(_geminiService);
      case AIProvider.openai:
        return OpenAIServiceAdapter(_openaiService);
      case AIProvider.ollama:
        return OllamaServiceAdapter(_ollamaService, _currentOllamaModel);
      case AIProvider.localOllama:
        return LocalOllamaServiceAdapter(_localOllamaService);
    }
  }
}