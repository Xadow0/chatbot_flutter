import 'package:flutter/foundation.dart';
import '../models/message_model.dart';
import '../models/ollama_models.dart';
import '../models/ollama_local_models.dart'; // CAMBIADO: nuevo import
import 'gemini_service.dart';
import 'ollama_service.dart';
import 'openai_service.dart';
import 'local_llm_service.dart';

enum AIProvider {
  gemini,
  ollama,
  openai,
  localLLM, // Ollama Local (ejecut√°ndose en la m√°quina del usuario)
}

class AIServiceSelector extends ChangeNotifier {
  final GeminiService _geminiService;
  final OllamaService _ollamaService;
  final OpenAIService _openaiService;
  final LocalLLMService _localLLMService;
  
  AIProvider _currentProvider = AIProvider.gemini;
  String _currentOllamaModel = 'phi3:latest';
  String _currentOpenAIModel = 'gpt-4o-mini';
  List<OllamaModel> _availableModels = [];
  bool _ollamaAvailable = false;
  bool _openaiAvailable = false;
  OllamaLocalStatus _localLLMStatus = OllamaLocalStatus.stopped; // CAMBIADO: nuevo tipo
  
  AIServiceSelector({
    required GeminiService geminiService,
    required OllamaService ollamaService,
    required OpenAIService openaiService,
    required LocalLLMService localLLMService,
  }) : _geminiService = geminiService,
       _ollamaService = ollamaService,
       _openaiService = openaiService,
       _localLLMService = localLLMService {
    _initializeServices();
    
    // Escuchar cambios de estado del LLM local
    _localLLMService.addStatusListener(_onLocalLLMStatusChanged);
  }
  
  // Getters
  AIProvider get currentProvider => _currentProvider;
  String get currentOllamaModel => _currentOllamaModel;
  String get currentOpenAIModel => _currentOpenAIModel;
  List<OllamaModel> get availableModels => _availableModels;
  List<String> get availableOpenAIModels => OpenAIService.availableModels;
  bool get ollamaAvailable => _ollamaAvailable;
  bool get openaiAvailable => _openaiAvailable;
  OllamaService get ollamaService => _ollamaService;
  OpenAIService get openaiService => _openaiService;
  ConnectionInfo get connectionInfo => _ollamaService.connectionInfo;
  
  // Getters para LLM local (Ollama Local)
  LocalLLMService get localLLMService => _localLLMService;
  OllamaLocalStatus get localLLMStatus => _localLLMStatus; // CAMBIADO: nuevo tipo
  bool get localLLMAvailable => _localLLMStatus == OllamaLocalStatus.ready; // CAMBIADO
  bool get localLLMLoading => _localLLMStatus == OllamaLocalStatus.connecting; // CAMBIADO
  String? get localLLMError => _localLLMService.errorMessage;
  
  // Stream de estado de conexi√≥n
  Stream<ConnectionInfo> get connectionStream => _ollamaService.connectionStream;
  
  // Callback para cambios de estado del LLM local
  void _onLocalLLMStatusChanged(OllamaLocalStatus status) { // CAMBIADO: nuevo tipo
    debugPrint('üì° [AIServiceSelector] Estado LLM local cambi√≥ a: ${status.displayText}');
    _localLLMStatus = status;
    notifyListeners();
  }
  
  // NUEVO: M√©todo p√∫blico para refrescar conexi√≥n y modelos de Ollama
  Future<void> refreshOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando Ollama...');
    try {
      await _ollamaService.reconnect();
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      }
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error refrescando Ollama: $e');
    }
  }
  
  // Inicializar todos los servicios
  Future<void> _initializeServices() async {
    debugPrint('üé¨ [AIServiceSelector] Inicializando servicios de IA...');
    
    // Inicializar Ollama (servidor remoto)
    await _initializeOllama();
    
    // Inicializar OpenAI
    _initializeOpenAI();
    
    // El LLM local se inicializa bajo demanda, no autom√°ticamente
    
    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados');
    debugPrint('   üìä Gemini: Siempre disponible');
    debugPrint('   üìä Ollama (remoto): ${_ollamaAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä OpenAI: ${_openaiAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä LLM Local (Ollama): ${_localLLMStatus.displayText}');
  }
  
  // Inicializar Ollama (servidor remoto)
  Future<void> _initializeOllama() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando Ollama remoto...');
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      } else {
        debugPrint('   ‚ö†Ô∏è Ollama remoto no disponible en la inicializaci√≥n');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama: $e');
    }
    notifyListeners();
  }
  
  // Inicializar OpenAI
  void _initializeOpenAI() {
    _openaiAvailable = _openaiService.isAvailable;
    if (_openaiAvailable) {
      debugPrint('‚úÖ [AIServiceSelector] OpenAI disponible');
      debugPrint('   ü§ñ Modelos disponibles: ${OpenAIService.availableModels.join(", ")}');
    } else {
      debugPrint('‚ö†Ô∏è [AIServiceSelector] OpenAI no disponible (API Key no configurada)');
    }
  }
  
  // MODIFICADO: Inicializar LLM Local (Ollama Local)
  Future<OllamaLocalInitResult> initializeLocalLLM() async { // CAMBIADO: nuevo tipo de retorno
    debugPrint('üöÄ [AIServiceSelector] Iniciando Ollama Local...');
    
    final result = await _localLLMService.initializeModel();
    
    if (result.success) {
      debugPrint('‚úÖ [AIServiceSelector] Ollama Local inicializado correctamente');
      debugPrint('   ü§ñ Modelo activo: ${result.modelName}');
      debugPrint('   üìã Modelos disponibles: ${result.availableModels?.join(", ")}');
    } else {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama Local: ${result.error}');
    }
    
    notifyListeners();
    return result;
  }
  
  // MODIFICADO: Detener LLM Local
  Future<void> stopLocalLLM() async {
    debugPrint('üõë [AIServiceSelector] Deteniendo Ollama Local...');
    
    // Si el proveedor actual es el LLM local, cambiar a Gemini
    if (_currentProvider == AIProvider.localLLM) {
      debugPrint('   üîÑ Cambiando a Gemini antes de detener');
      await setProvider(AIProvider.gemini);
    }
    
    await _localLLMService.stopModel();
    notifyListeners();
  }
  
  // MODIFICADO: Reintentar inicializaci√≥n del LLM local
  Future<OllamaLocalInitResult> retryLocalLLM() async { // CAMBIADO: nuevo tipo de retorno
    debugPrint('üîÑ [AIServiceSelector] Reintentando inicializaci√≥n de Ollama Local...');
    return await _localLLMService.retry();
  }
  
  // Verificar disponibilidad de Ollama (servidor remoto)
  Future<void> _checkOllamaAvailability() async {
    try {
      debugPrint('üíì [AIServiceSelector] Verificando disponibilidad de Ollama remoto...');
      final health = await _ollamaService.checkHealth();
      _ollamaAvailable = health.success && health.ollamaAvailable;
      debugPrint('   ${_ollamaAvailable ? "‚úÖ" : "‚ùå"} Ollama remoto ${_ollamaAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('   ‚ùå Error en health check: $e');
      _ollamaAvailable = false;
    }
  }
  
  // Cargar modelos disponibles (Ollama remoto)
  Future<void> _loadAvailableModels() async {
    try {
      debugPrint('üìã [AIServiceSelector] Cargando modelos de Ollama remoto...');
      _availableModels = await _ollamaService.getModels();
      
      // Si el modelo actual no est√° disponible, seleccionar el primero
      if (_availableModels.isNotEmpty && 
          !_availableModels.any((m) => m.name == _currentOllamaModel)) {
        final oldModel = _currentOllamaModel;
        _currentOllamaModel = _availableModels.first.name;
        debugPrint('   ‚ö†Ô∏è Modelo $oldModel no encontrado, usando ${_availableModels.first.name}');
      } else {
        debugPrint('   ‚úÖ Modelo actual $_currentOllamaModel est√° disponible');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error cargando modelos: $e');
      _availableModels = [];
    }
  }
  
  // Cambiar proveedor
  Future<void> setProvider(AIProvider provider) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando proveedor a: $provider');
    
    // Validaciones seg√∫n el proveedor
    if (provider == AIProvider.ollama) {
      await _checkOllamaAvailability();
      if (!_ollamaAvailable) {
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a Ollama: no disponible');
        throw Exception('Ollama no est√° disponible. Verifica que el servidor est√© accesible.');
      }
    } else if (provider == AIProvider.openai) {
      if (!_openaiAvailable) {
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a OpenAI: no disponible');
        throw Exception('OpenAI no est√° disponible. Verifica tu API Key.');
      }
    } else if (provider == AIProvider.localLLM) {
      if (_localLLMStatus != OllamaLocalStatus.ready) { // CAMBIADO
        debugPrint('‚ùå [AIServiceSelector] No se puede cambiar a Ollama Local: no disponible');
        throw Exception('Ollama Local no est√° disponible. Inicializa primero.');
      }
    }
    
    _currentProvider = provider;
    notifyListeners();
    debugPrint('‚úÖ [AIServiceSelector] Proveedor cambiado a: $provider');
  }
  
  // Cambiar modelo de Ollama remoto
  Future<void> setOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo de Ollama a: $modelName');
    
    // Verificar que el modelo est√© disponible
    final isAvailable = await _ollamaService.isModelAvailable(modelName);
    if (!isAvailable) {
      debugPrint('‚ùå [AIServiceSelector] Modelo no disponible: $modelName');
      throw Exception('Modelo $modelName no disponible');
    }
    
    _currentOllamaModel = modelName;
    notifyListeners();
    debugPrint('‚úÖ [AIServiceSelector] Modelo de Ollama cambiado a: $modelName');
  }
  
  // Cambiar modelo de OpenAI
  void setOpenAIModel(String modelName) {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo de OpenAI a: $modelName');
    _currentOpenAIModel = modelName;
    notifyListeners();
  }
  
  // Enviar mensaje al proveedor actual
  Future<String> sendMessage(String message, {List<Message>? history}) async {
    debugPrint('üì® [AIServiceSelector] === ENVIANDO MENSAJE ===');
    debugPrint('   üéØ Proveedor: $_currentProvider');
    debugPrint('   üí¨ Mensaje: ${message.length > 50 ? "${message.substring(0, 50)}..." : message}');
    debugPrint('   üìö Historial: ${history?.length ?? 0} mensajes');
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        return await _sendToGemini(message, history);
      case AIProvider.ollama:
        return await _sendToOllama(message, history);
      case AIProvider.openai:
        return await _sendToOpenAI(message, history);
      case AIProvider.localLLM:
        return await _sendToLocalLLM(message, history);
    }
  }
  
  // Enviar a Gemini
  Future<String> _sendToGemini(String message, List<Message>? history) async {
    try {
      debugPrint('   üíé Usando Gemini...');
      final response = await _geminiService.generateContent(message);
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Gemini recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Gemini: $e');
      throw Exception('Error con Gemini: $e');
    }
  }
  
  // Enviar a Ollama (servidor remoto)
  Future<String> _sendToOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando disponibilidad del modelo $_currentOllamaModel...');
      
      // Verificar que el modelo est√© disponible
      final isAvailable = await _ollamaService.isModelAvailable(_currentOllamaModel);
      if (!isAvailable) {
        debugPrint('   ‚ùå Modelo $_currentOllamaModel no disponible');
        throw Exception('Modelo $_currentOllamaModel no disponible');
      }
      
      debugPrint('   ‚úì Modelo disponible');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        final chatMessages = _convertHistoryToChatMessages(history, message);
        response = await _ollamaService.chatWithHistory(
          model: _currentOllamaModel,
          messages: chatMessages,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _ollamaService.generateResponse(
          model: _currentOllamaModel,
          prompt: message,
          systemPrompt: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting.',
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama: $e');
      throw Exception('Error con Ollama: $e');
    }
  }
  
  // Enviar a OpenAI
  Future<String> _sendToOpenAI(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Usando modelo: $_currentOpenAIModel');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        final messages = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          messages.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        messages.add({
          'role': 'user',
          'content': message,
        });
        
        response = await _openaiService.chatWithHistory(
          messages: messages,
          model: _currentOpenAIModel,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _openaiService.generateContent(
          message,
          model: _currentOpenAIModel,
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de OpenAI recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con OpenAI: $e');
      throw Exception('Error con OpenAI: $e');
    }
  }
  
  // MODIFICADO: Enviar a Ollama Local
  Future<String> _sendToLocalLLM(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando estado de Ollama Local...');
      
      if (_localLLMStatus != OllamaLocalStatus.ready) { // CAMBIADO
        debugPrint('   ‚ùå Ollama Local no est√° listo: ${_localLLMStatus.displayText}');
        throw Exception('Ollama Local no est√° listo');
      }
      
      debugPrint('   ‚úì Ollama Local disponible');
      debugPrint('   üí≠ Generando respuesta localmente...');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        // Convertir historial al formato correcto
        final chatHistory = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          chatHistory.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        response = await _localLLMService.chatWithHistory(
          prompt: message,
          history: chatHistory,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _localLLMService.generateContent(message);
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama Local recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama Local: $e');
      
      // Proporcionar informaci√≥n de diagn√≥stico
      debugPrint('üí° [AIServiceSelector] DIAGN√ìSTICO:');
      debugPrint('   1. Verifica que Ollama est√© ejecut√°ndose (ollama serve)');
      debugPrint('   2. Comprueba que el modelo est√© descargado (ollama list)');
      debugPrint('   3. Intenta con un prompt m√°s corto');
      
      throw Exception('Error con Ollama Local: $e');
    }
  }
  
  // Convertir historial a formato de chat para Ollama remoto
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];
    
    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;
    
    debugPrint('   üìö Convirtiendo historial: ${recentHistory.length} mensajes recientes');

    for (final msg in recentHistory) {
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text,
      ));
    }
    
    messages.add(ChatMessage(
      role: 'user',
      content: newMessage,
    ));
    
    debugPrint('   ‚úì Total de mensajes para chat: ${messages.length}');
    
    return messages;
  }
  
  @override
  void dispose() {
    debugPrint('üî¥ [AIServiceSelector] Disposing...');
    _localLLMService.removeStatusListener(_onLocalLLMStatusChanged);
    _localLLMService.dispose();
    _ollamaService.dispose();
    super.dispose();
  }
}