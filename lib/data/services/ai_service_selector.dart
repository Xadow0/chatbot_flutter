import 'package:flutter/foundation.dart';
import '../models/message_model.dart';
import '../models/ollama_models.dart';
import '../models/local_ollama_models.dart';
import 'gemini_service.dart';
import 'ollama_service.dart';
import 'openai_service.dart';
import 'local_ollama_service.dart';

enum AIProvider {
  gemini,
  ollama,
  openai,
  localOllama,
}

class AIServiceSelector extends ChangeNotifier {
  final GeminiService _geminiService;
  final OllamaService _ollamaService;
  final OpenAIService _openaiService;
  final OllamaManagedService _localOllamaService;
  
  AIProvider _currentProvider = AIProvider.gemini;
  String _currentOllamaModel = 'phi3:latest';
  String _currentOpenAIModel = 'gpt-4o-mini';
  List<OllamaModel> _availableModels = [];
  bool _ollamaAvailable = false;
  bool _openaiAvailable = false;
  LocalOllamaStatus _localOllamaStatus = LocalOllamaStatus.notInitialized;
  
  AIServiceSelector({
    required GeminiService geminiService,
    required OllamaService ollamaService,
    required OpenAIService openaiService,
    required OllamaManagedService localOllamaService,
  }) : _geminiService = geminiService,
       _ollamaService = ollamaService,
       _openaiService = openaiService,
       _localOllamaService = localOllamaService {
    _initializeServices();
    _localOllamaService.addStatusListener(_onLocalOllamaStatusChanged);
  }
  
  // Getters
  AIProvider get currentProvider => _currentProvider;
  String get currentOllamaModel => _currentOllamaModel;
  String get currentOpenAIModel => _currentOpenAIModel;
  List<OllamaModel> get availableModels => _availableModels;
  List<String> get availableOpenAIModels => OpenAIService.availableModels;
  bool get ollamaAvailable => _ollamaAvailable;
  bool get openaiAvailable => _openaiAvailable;
  OllamaService get ollamaService => _ollamaService;
  OpenAIService get openaiService => _openaiService;
  ConnectionInfo get connectionInfo => _ollamaService.connectionInfo;
  
  // Getters para Ollama Local
  OllamaManagedService get localOllamaService => _localOllamaService;
  LocalOllamaStatus get localOllamaStatus => _localOllamaStatus;
  bool get localOllamaAvailable => _localOllamaStatus == LocalOllamaStatus.ready;
  bool get localOllamaLoading => _localOllamaStatus.isProcessing;
  String? get localOllamaError => _localOllamaService.errorMessage;
  
  Stream<ConnectionInfo> get connectionStream => _ollamaService.connectionStream;
  
  void _onLocalOllamaStatusChanged(LocalOllamaStatus status) {
    debugPrint('üì° [AIServiceSelector] Estado Ollama Local cambi√≥ a: ${status.displayText}');
    _localOllamaStatus = status;
    notifyListeners();
  }
  
  Future<void> refreshOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Refrescando Ollama...');
    try {
      await _ollamaService.reconnect();
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      }
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error refrescando Ollama: $e');
    }
  }
  
  Future<void> _initializeServices() async {
    debugPrint('üé¨ [AIServiceSelector] Inicializando servicios de IA...');
    
    await _initializeOllama();
    _initializeOpenAI();
    
    debugPrint('‚úÖ [AIServiceSelector] Servicios inicializados');
    debugPrint('   üìä Gemini: Siempre disponible');
    debugPrint('   üìä Ollama (remoto): ${_ollamaAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä OpenAI: ${_openaiAvailable ? "Disponible" : "No disponible"}');
    debugPrint('   üìä Ollama Local: ${_localOllamaStatus.displayText}');
  }
  
  Future<void> _initializeOllama() async {
    try {
      debugPrint('üî∑ [AIServiceSelector] Inicializando Ollama remoto...');
      await _checkOllamaAvailability();
      if (_ollamaAvailable) {
        await _loadAvailableModels();
      } else {
        debugPrint('   ‚ö†Ô∏è Ollama remoto no disponible en la inicializaci√≥n');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama: $e');
    }
    notifyListeners();
  }
  
  void _initializeOpenAI() {
    _openaiAvailable = _openaiService.isAvailable;
    if (_openaiAvailable) {
      debugPrint('‚úÖ [AIServiceSelector] OpenAI disponible');
      debugPrint('   ü§ñ Modelos disponibles: ${OpenAIService.availableModels.join(", ")}');
    } else {
      debugPrint('‚ö†Ô∏è [AIServiceSelector] OpenAI no disponible (API Key no configurada)');
    }
  }
  
  Future<LocalOllamaInitResult> initializeLocalOllama() async {
    debugPrint('üöÄ [AIServiceSelector] Iniciando Ollama Local...');
    
    final result = await _localOllamaService.initialize();
    
    if (result.success) {
      debugPrint('‚úÖ [AIServiceSelector] Ollama Local inicializado correctamente');
      debugPrint('   ü§ñ Modelo activo: ${result.modelName}');
      debugPrint('   üìã Modelos disponibles: ${result.availableModels?.join(", ")}');
    } else {
      debugPrint('‚ùå [AIServiceSelector] Error inicializando Ollama Local: ${result.error}');
    }
    
    notifyListeners();
    return result;
  }
  
  Future<void> stopLocalOllama() async {
    debugPrint('üõë [AIServiceSelector] Deteniendo Ollama Local...');
    
    if (_currentProvider == AIProvider.localOllama) {
      debugPrint('   üîÑ Cambiando a Gemini antes de detener');
      await setProvider(AIProvider.gemini);
    }
    
    await _localOllamaService.stop();
    notifyListeners();
  }
  
  Future<LocalOllamaInitResult> retryLocalOllama() async {
    debugPrint('üîÑ [AIServiceSelector] Reintentando inicializaci√≥n de Ollama Local...');
    return await _localOllamaService.retry();
  }
  
  Future<void> _checkOllamaAvailability() async {
    try {
      debugPrint('üíì [AIServiceSelector] Verificando disponibilidad de Ollama remoto...');
      final health = await _ollamaService.checkHealth();
      _ollamaAvailable = health.success && health.ollamaAvailable;
      debugPrint('   ${_ollamaAvailable ? "‚úÖ" : "‚ùå"} Ollama remoto ${_ollamaAvailable ? "disponible" : "no disponible"}');
    } catch (e) {
      debugPrint('   ‚ùå Error en health check: $e');
      _ollamaAvailable = false;
    }
  }
  
  Future<void> _loadAvailableModels() async {
    try {
      debugPrint('üìã [AIServiceSelector] Cargando modelos de Ollama remoto...');
      _availableModels = await _ollamaService.getModels();
      
      if (_availableModels.isNotEmpty && 
          !_availableModels.any((m) => m.name == _currentOllamaModel)) {
        final oldModel = _currentOllamaModel;
        _currentOllamaModel = _availableModels.first.name;
        debugPrint('   ‚ö†Ô∏è Modelo $oldModel no encontrado, usando ${_availableModels.first.name}');
      } else {
        debugPrint('   ‚úÖ Modelo actual $_currentOllamaModel est√° disponible');
      }
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error cargando modelos: $e');
      _availableModels = [];
    }
  }
  
  Future<void> setProvider(AIProvider provider) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando proveedor a: $provider');
    
    if (provider == AIProvider.ollama && !_ollamaAvailable) {
      debugPrint('   ‚ö†Ô∏è Ollama remoto no est√° disponible');
      throw Exception('Ollama remoto no est√° disponible');
    }
    
    if (provider == AIProvider.openai && !_openaiAvailable) {
      debugPrint('   ‚ö†Ô∏è OpenAI no est√° disponible');
      throw Exception('OpenAI no est√° disponible. Configure API Key en .env');
    }
    
    if (provider == AIProvider.localOllama && !localOllamaAvailable) {
      debugPrint('   ‚ö†Ô∏è Ollama Local no est√° listo');
      throw Exception('Ollama Local no est√° listo. Inicial√≠zalo primero.');
    }
    
    _currentProvider = provider;
    notifyListeners();
    debugPrint('   ‚úÖ Proveedor cambiado a $provider');
  }
  
  Future<void> setOllamaModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo Ollama a: $modelName');
    
    if (!_availableModels.any((m) => m.name == modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no disponible');
      throw Exception('Modelo no disponible');
    }
    
    _currentOllamaModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo Ollama cambiado a $modelName');
  }
  
  Future<void> setOpenAIModel(String modelName) async {
    debugPrint('üîÑ [AIServiceSelector] Cambiando modelo OpenAI a: $modelName');
    
    if (!OpenAIService.availableModels.contains(modelName)) {
      debugPrint('   ‚ùå Modelo $modelName no disponible');
      throw Exception('Modelo no disponible');
    }
    
    _currentOpenAIModel = modelName;
    notifyListeners();
    debugPrint('   ‚úÖ Modelo OpenAI cambiado a $modelName');
  }
  
  Future<String> sendMessage(String message, {List<Message>? history}) async {
    debugPrint('üì§ [AIServiceSelector] === ENVIANDO MENSAJE ===');
    debugPrint('   üéØ Proveedor: $_currentProvider');
    debugPrint('   üí¨ Mensaje: ${message.length > 50 ? "${message.substring(0, 50)}..." : message}');
    debugPrint('   üìö Historial: ${history?.length ?? 0} mensajes');
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        return await _sendToGemini(message, history);
      case AIProvider.ollama:
        return await _sendToOllama(message, history);
      case AIProvider.openai:
        return await _sendToOpenAI(message, history);
      case AIProvider.localOllama:
        return await _sendToLocalOllama(message, history);
    }
  }
  
  Future<String> _sendToGemini(String message, List<Message>? history) async {
    try {
      debugPrint('   üíé Usando Gemini...');
      final response = await _geminiService.generateContent(message);
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Gemini recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Gemini: $e');
      throw Exception('Error con Gemini: $e');
    }
  }
  
  Future<String> _sendToOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando disponibilidad del modelo $_currentOllamaModel...');
      
      final isAvailable = await _ollamaService.isModelAvailable(_currentOllamaModel);
      if (!isAvailable) {
        debugPrint('   ‚ùå Modelo $_currentOllamaModel no disponible');
        throw Exception('Modelo $_currentOllamaModel no disponible');
      }
      
      debugPrint('   ‚úì Modelo disponible');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        final chatMessages = _convertHistoryToChatMessages(history, message);
        response = await _ollamaService.chatWithHistory(
          model: _currentOllamaModel,
          messages: chatMessages,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _ollamaService.generateResponse(
          model: _currentOllamaModel,
          prompt: message,
          systemPrompt: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting.',
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama: $e');
      throw Exception('Error con Ollama: $e');
    }
  }
  
  Future<String> _sendToOpenAI(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Usando modelo: $_currentOpenAIModel');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        final messages = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          messages.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        messages.add({
          'role': 'user',
          'content': message,
        });
        
        response = await _openaiService.chatWithHistory(
          messages: messages,
          model: _currentOpenAIModel,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _openaiService.generateContent(
          message,
          model: _currentOpenAIModel,
        );
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de OpenAI recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con OpenAI: $e');
      throw Exception('Error con OpenAI: $e');
    }
  }
  
  Future<String> _sendToLocalOllama(String message, List<Message>? history) async {
    try {
      debugPrint('   üîç Verificando estado de Ollama Local...');
      
      if (_localOllamaStatus != LocalOllamaStatus.ready) {
        debugPrint('   ‚ùå Ollama Local no est√° listo: ${_localOllamaStatus.displayText}');
        throw Exception('Ollama Local no est√° listo');
      }
      
      debugPrint('   ‚úì Ollama Local disponible');
      debugPrint('   üí≠ Generando respuesta localmente...');
      
      String response;
      if (history != null && history.isNotEmpty) {
        debugPrint('   üìù Usando chat con historial (${history.length} mensajes)');
        
        final chatHistory = <Map<String, String>>[];
        final recentHistory = history.length > 10 
            ? history.sublist(history.length - 10) 
            : history;
        
        for (final msg in recentHistory) {
          chatHistory.add({
            'role': msg.isUser ? 'user' : 'assistant',
            'content': msg.text,
          });
        }
        
        response = await _localOllamaService.chatWithHistory(
          prompt: message,
          history: chatHistory,
        );
      } else {
        debugPrint('   üí≠ Usando generaci√≥n simple');
        response = await _localOllamaService.generateContent(message);
      }
      
      debugPrint('‚úÖ [AIServiceSelector] Respuesta de Ollama Local recibida (${response.length} chars)');
      debugPrint('üü¢ [AIServiceSelector] === ENV√çO EXITOSO ===\n');
      return response;
    } catch (e) {
      debugPrint('‚ùå [AIServiceSelector] Error con Ollama Local: $e');
      throw Exception('Error con Ollama Local: $e');
    }
  }
    
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];
    
    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;
    
    debugPrint('   üìö Convirtiendo historial: ${recentHistory.length} mensajes recientes');

    for (final msg in recentHistory) {
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.text,
      ));
    }
    
    messages.add(ChatMessage(
      role: 'user',
      content: newMessage,
    ));
    
    debugPrint('   ‚úì Total de mensajes para chat: ${messages.length}');
    
    return messages;
  }
  
  @override
  void dispose() {
    debugPrint('üî¥ [AIServiceSelector] Disposing...');
    _localOllamaService.removeStatusListener(_onLocalOllamaStatusChanged);
    _localOllamaService.dispose();
    _ollamaService.dispose();
    super.dispose();
  }
}