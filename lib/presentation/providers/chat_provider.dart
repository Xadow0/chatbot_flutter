import 'package:flutter/foundation.dart';
import 'package:shared_preferences/shared_preferences.dart';
import 'dart:io';
import '../../domain/entities/message_entity.dart';
import '../../domain/entities/quick_response_entity.dart';
import '../../data/models/quick_response_model.dart';
import '../../data/models/remote_ollama_models.dart';
import '../../data/models/local_ollama_models.dart';
import '../../data/services/gemini_service.dart';
import '../../data/services/ollama_service.dart';
import '../../data/services/openai_service.dart';
import '../../data/services/local_ollama_service.dart';
import '../../data/services/ai_service_selector.dart';
import '../../data/services/preferences_service.dart';
import '../../data/services/ai_service_adapters.dart';
import '../../domain/usecases/command_processor.dart';
import '../../domain/usecases/send_message_usecase.dart';
import '../../domain/repositories/chat_repository.dart';
import '../../domain/repositories/conversation_repository.dart';
import '../../domain/repositories/command_repository.dart'; 
import '../../core/constants/commands_help.dart';
import 'command_management_provider.dart';

class ChatProvider extends ChangeNotifier {
  // ============================================================================
  // ESTADO INTERNO: ENTIDADES (Domain Layer)
  // ============================================================================
  final List<MessageEntity> _messages = [];
  List<QuickResponseEntity> _quickResponses =
      QuickResponseProvider.defaultResponsesAsEntities;
  bool _isProcessing = false;
  bool _isStreaming = false;
  bool _isNewConversation = true;
  bool _isRetryingOllama = false;
  // Controla si el usuario puede seleccionar Ollama remoto desde la UI.
  bool _ollamaSelectable = true;
  bool _needsHistoryLoad = false;

  bool _hasUnsavedChanges = false; // Para saber si hay algo que guardar
  File? _currentConversationFile; // Para saber qu√© archivo sobrescribir/borrar
  bool _isSaving = false;

  late SendMessageUseCase _sendMessageUseCase; // No final - se actualiza al cambiar proveedor
  late final AIServiceSelector _aiSelector;
  late final PreferencesService _preferencesService;

  // INTERFACES DE REPOSITORIO
  final ChatRepository _chatRepository;
  final ConversationRepository _conversationRepository;
  final CommandRepository _commandRepository; 

  bool Function()? _getSyncStatus;
  
  // Referencia al CommandManagementProvider para obtener carpetas y preferencias
  CommandManagementProvider? _commandManagementProvider;

  // Referencias a los servicios
  late final GeminiService _geminiService;
  late final OllamaService _ollamaService;
  late final OpenAIService _openaiService;
  late final OllamaManagedService _localOllamaService;

  // Adaptadores
  late final GeminiServiceAdapter _geminiAdapter;
  late OllamaServiceAdapter _ollamaAdapter; // No final porque se recrea
  late final OpenAIServiceAdapter _openaiAdapter;
  late final LocalOllamaServiceAdapter _localOllamaAdapter;

  late CommandProcessor _commandProcessor; // No final - se actualiza al cambiar proveedor

  bool _showModelSelector = false;
  List<OllamaModel> _availableModels = [];
  String _currentModel = 'phi3:latest';
  AIProvider _currentProvider = AIProvider.gemini;

  // ==========================================================================
  // Clave para preferencia de agrupar comandos del sistema (fallback)
  // ==========================================================================
  static const String _groupSystemCommandsKey = 'group_system_commands';

  ChatProvider({
    required ChatRepository chatRepository,
    required ConversationRepository conversationRepository,
    required CommandRepository commandRepository, 
    required AIServiceSelector aiServiceSelector,
  })  : _chatRepository = chatRepository,
        _conversationRepository = conversationRepository,
        _commandRepository = commandRepository,
        _aiSelector = aiServiceSelector,
        _geminiService = aiServiceSelector.geminiService,
        _ollamaService = aiServiceSelector.ollamaService,
        _openaiService = aiServiceSelector.openaiService,
        _localOllamaService = aiServiceSelector.localOllamaService {
    
    // Crear adaptadores
    _geminiAdapter = GeminiServiceAdapter(_geminiService);
    _ollamaAdapter = OllamaServiceAdapter(_ollamaService, _currentModel);
    _openaiAdapter = OpenAIServiceAdapter(_openaiService);
    _localOllamaAdapter = LocalOllamaServiceAdapter(_localOllamaService);

    _preferencesService = PreferencesService();

    // Suscribirse a los cambios del selector
    _aiSelector.addListener(_onAiSelectorChanged);

    // Inicializar CommandProcessor con Gemini por defecto
    _commandProcessor = CommandProcessor(_geminiAdapter, _commandRepository);

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: _chatRepository,
    );

    _initializeModels();
  }

  /// Vincula el CommandManagementProvider para obtener carpetas y preferencias
  void setCommandManagementProvider(CommandManagementProvider provider) {
    // Si ya hab√≠a uno vinculado, nos desuscribimos para evitar fugas de memoria
    if (_commandManagementProvider != null) {
      _commandManagementProvider!.removeListener(_onCommandDataChanged);
    }

    _commandManagementProvider = provider;
    
    // 1. SOLUCI√ìN PROFESIONAL: Suscripci√≥n completa.
    // Escuchamos cualquier cambio (notificaci√≥n) que emita el proveedor de comandos.
    provider.addListener(_onCommandDataChanged);
    
    // 2. Sincronizaci√≥n inicial inmediata
    // Si el proveedor ya tiene datos cargados, los aplicamos ya mismo.
    if (!provider.isLoading) {
      _onCommandDataChanged();
    }
    
    debugPrint('‚úÖ [ChatProvider] Vinculado reactivamente a CommandManagementProvider');
  }

  /// Esta funci√≥n se ejecuta AUTOM√ÅTICAMENTE cada vez que CommandManagementProvider hace notifyListeners()
  void _onCommandDataChanged() {
    // Evitamos actualizaciones innecesarias si el proveedor est√° cargando (opcional, seg√∫n preferencia visual)
    // Pero para la carga inicial, queremos que se ejecute al finalizar la carga.
    if (_commandManagementProvider == null) return;

    // Actualizamos las QuickResponses bas√°ndonos en el estado ACTUAL del proveedor de comandos
    _updateQuickResponsesFromProvider();
  }

  /// M√©todo s√≠ncrono y r√°pido para reconstruir las respuestas desde el proveedor vinculado
  void _updateQuickResponsesFromProvider() {
    if (_commandManagementProvider == null) return;

    final provider = _commandManagementProvider!;
    
    // Usamos el helper est√°tico para regenerar la lista
    final organizedResponses = QuickResponseProvider.buildOrganizedResponses(
      commands: provider.commands,
      folders: provider.folders,
      groupSystemCommands: provider.groupSystemCommands, // AQU√ç LEEMOS EL VALOR REAL ACTUALIZADO
    );

    _quickResponses = organizedResponses.map((r) => r.toEntity()).toList();
    
    // Notificamos a la UI del Chat para que se repinte
    notifyListeners();
  }

  // M√©todo para manejar las notificaciones del CommandManagementProvider
  void _onCommandProviderUpdated() {
    // Solo actualizamos si no est√° cargando, para evitar parpadeos innecesarios durante la carga
    // Opcional: puedes quitar el if si quieres ver actualizaciones en tiempo real
    if (_commandManagementProvider != null && !_commandManagementProvider!.isLoading) {
       refreshQuickResponses();
    }
  }

  /// Vincula el estado de sincronizaci√≥n desde AuthProvider
  void setSyncStatusChecker(bool Function() checker) {
    _getSyncStatus = checker;
  }

  /// Escucha los cambios de AIServiceSelector y notifica a los listeners de ChatProvider
  Future<void> _onAiSelectorChanged() async {
    debugPrint('üîÑ [ChatProvider] AIServiceSelector notific√≥ cambios, actualizando UI...');

    // 1. Sincronizar la lista de modelos disponibles
    if (_aiSelector.ollamaAvailable) {
      if (!_ollamaSelectable) {
        _ollamaSelectable = true;
        debugPrint('   üîì Ollama disponible: desbloqueando selecci√≥n en la UI');
      }
      
      if (!listEquals(_availableModels, _aiSelector.availableModels)) {
        _availableModels = _aiSelector.availableModels;
        debugPrint(
            '   ‚úÖ Lista de modelos Ollama (remoto) actualizada: ${_availableModels.length} modelos');

        if (_availableModels.isNotEmpty) {
          final currentModelExists =
              _availableModels.any((m) => m.name == _currentModel);

          if (!currentModelExists || _currentModel.isEmpty) {
            _currentModel = _availableModels.first.name;
            _ollamaAdapter.updateModel(_currentModel);
            debugPrint(
                '   ‚ö†Ô∏è Modelo actual no encontrado. Seleccionando por defecto: $_currentModel');
          }
        }
      }

      if (_currentProvider == AIProvider.ollama &&
          _currentModel != _aiSelector.currentOllamaModel) {
        _currentModel = _aiSelector.currentOllamaModel;
        _ollamaAdapter.updateModel(_currentModel);
      }
    } else {
      if (_availableModels.isNotEmpty) {
        _availableModels = [];
        debugPrint('   ‚ùå Ollama (remoto) desconectado. Vaciando lista de modelos.');
      }

      if (_ollamaSelectable) {
        _ollamaSelectable = false;
        debugPrint('   üîí Bloqueando selecci√≥n de Ollama en la UI (desconectado)');
      }

      if (_currentProvider == AIProvider.ollama) {
        debugPrint('   ‚ö†Ô∏è ¬°Ollama (remoto) era el proveedor activo y se ha desconectado!');
        debugPrint('   üîÑ Cambiando autom√°ticamente a Gemini por defecto...');

        await selectProvider(AIProvider.gemini);

        debugPrint('   ‚úÖ [AIServiceSelector change] Cambio a Gemini completado.');
        
        _addOllamaConnectionErrorMessage();
      }
    }
    notifyListeners();
  }

  /// A√±ade un mensaje de error al chat cuando Ollama (remoto) se desconecta
  void _addOllamaConnectionErrorMessage() {
    final errorMessage = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: '‚ùå El servidor de Ollama (remoto) se ha desconectado.\n\n'
          'Se ha cambiado autom√°ticamente a **Gemini**.\n\n'
          'Por favor, comprueba la conexi√≥n del servidor e int√©ntalo de nuevo m√°s tarde.',
      type: MessageTypeEntity.bot,
      timestamp: DateTime.now(),
    );

    if (_messages.isEmpty ||
        (_messages.last.type != MessageTypeEntity.bot) ||
        (_messages.last.content != errorMessage.content)) {
      _messages.add(errorMessage);
    }
  }

  /// Actualiza el CommandProcessor seg√∫n el proveedor actual
  void _updateCommandProcessor() {
    AIServiceBase currentAdapter;

    switch (_currentProvider) {
      case AIProvider.gemini:
        currentAdapter = _geminiAdapter;
        debugPrint('   üîµ Usando GeminiAdapter');
        break;
      case AIProvider.ollama:
        _ollamaAdapter.updateModel(_currentModel);
        currentAdapter = _ollamaAdapter;
        debugPrint(
            '   üü™ Usando OllamaAdapter (remoto) con modelo: $_currentModel');
        break;
      case AIProvider.openai:
        currentAdapter = _openaiAdapter;
        debugPrint('   üü¢ Usando OpenAIAdapter');
        break;
      case AIProvider.localOllama:
        currentAdapter = _localOllamaAdapter;
        debugPrint('   üü† Usando LocalLLMAdapter (Ollama Embebido)');
        break;
    }

    // Crear nuevo CommandProcessor pasando el Repositorio
    _commandProcessor = CommandProcessor(currentAdapter, _commandRepository);

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: _chatRepository,
    );

    debugPrint(
        'üîÑ [ChatProvider] CommandProcessor actualizado para: $_currentProvider');
  }

  // ============================================================================
  // GETTERS: EXPONE ENTIDADES A LA UI
  // ============================================================================
  List<MessageEntity> get messages => List.unmodifiable(_messages);
  List<QuickResponseEntity> get quickResponses => _quickResponses;
  bool get isProcessing => _isProcessing;
  bool get isStreaming => _isStreaming;
  bool get showModelSelector => _showModelSelector;
  List<OllamaModel> get availableModels => _availableModels;
  String get currentModel => _currentModel;
  AIProvider get currentProvider => _currentProvider;
  ConnectionInfo get connectionInfo => _aiSelector.connectionInfo;
  bool get ollamaAvailable => _aiSelector.ollamaAvailable && _ollamaSelectable;
  bool get isRetryingOllama => _isRetryingOllama;
    bool get hasUnsavedChanges => _hasUnsavedChanges;

  AIServiceSelector get aiSelector => _aiSelector;
  bool get openaiAvailable => _aiSelector.openaiAvailable;
  String get currentOpenAIModel => _aiSelector.currentOpenAIModel;
  List<String> get availableOpenAIModels => _aiSelector.availableOpenAIModels;

  LocalOllamaStatus get localOllamaStatus => _aiSelector.localOllamaStatus;
  bool get localOllamaAvailable => _aiSelector.localOllamaAvailable;
  bool get localOllamaLoading => _aiSelector.localOllamaLoading;

  Stream<ConnectionInfo> get connectionStream => _aiSelector.connectionStream;

  // ============================================================================
  // M√âTODOS PARA GESTI√ìN DE HISTORIAL (para HistoryPage)
  // ============================================================================

  Future<List<FileSystemEntity>> listConversations() {
    return _conversationRepository.listConversations();
  }

  Future<void> _initializeModels() async {
    try {
      debugPrint('üé¨ [ChatProvider] Inicializando modelos...');

      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        if (_availableModels.isNotEmpty) {
          _currentModel = _availableModels.first.name;
          _ollamaAdapter.updateModel(_currentModel);
        }
      }

      await _restoreUserPreferences();
      
      // Cargar quick responses iniciales (incluye comandos del usuario)
      await _updateQuickResponses();

      _ollamaSelectable = _aiSelector.ollamaAvailable;

      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error inicializando modelos: $e');
    }
  }

  Future<void> _restoreUserPreferences() async {
    try {
      debugPrint('üîÑ [ChatProvider] Restaurando preferencias...');

      final lastProvider = await _preferencesService.getLastProvider();

      if (lastProvider != null) {
        bool canRestore = false;

        switch (lastProvider) {
          case AIProvider.gemini:
            canRestore = true;
            break;
          case AIProvider.ollama:
            canRestore = _aiSelector.ollamaAvailable;
            if (!canRestore) {
              debugPrint(
                  '   ‚ö†Ô∏è Ollama (remoto) no disponible, usando Gemini por defecto');
            }
            break;
          case AIProvider.openai:
            canRestore = _aiSelector.openaiAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è OpenAI no disponible, usando Gemini por defecto');
            }
            break;
          case AIProvider.localOllama:
            canRestore = _aiSelector.localOllamaAvailable;
            if (!canRestore) {
              debugPrint(
                  '   ‚ö†Ô∏è Ollama Embebido no disponible, usando Gemini por defecto');
            }
            break;
        }

        if (canRestore) {
          _currentProvider = lastProvider;
          _updateCommandProcessor();
          debugPrint('   ‚úÖ Proveedor restaurado: $lastProvider');
        }
      }
    } catch (e) {
      debugPrint('   ‚ùå Error restaurando preferencias: $e');
    }
  }

  void toggleModelSelector() {
    debugPrint(
        'üîÑ [ChatProvider] Toggling model selector: $_showModelSelector -> ${!_showModelSelector}');
    _showModelSelector = !_showModelSelector;
    notifyListeners();
  }

  void hideModelSelector() {
    if (_showModelSelector) {
      debugPrint('üîÑ [ChatProvider] Hiding model selector');
      _showModelSelector = false;
      notifyListeners();
    }
  }

  Future<void> selectModel(String modelName) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo a: $modelName');

    try {
      _currentModel = modelName;
      _aiSelector.setOllamaModel(modelName);
      _ollamaAdapter.updateModel(modelName);
      hideModelSelector();
      notifyListeners();
      debugPrint('   ‚úÖ Modelo cambiado a: $modelName');
    } catch (e) {
      debugPrint('   ‚ùå Error al cambiar modelo: $e');
      _currentModel = _aiSelector.currentOllamaModel;
      notifyListeners();
    }
  }

  Future<void> selectProvider(AIProvider provider) async {
    debugPrint('üîÑ [ChatProvider] Cambiando proveedor a: $provider');
    
    if (provider == AIProvider.ollama && !_aiSelector.ollamaAvailable) {
      debugPrint('   ‚ùå Ollama (remoto) no disponible. No se puede seleccionar por ahora.');
      return;
    }
    
    bool isAvailable = false;
    switch (provider) {
      case AIProvider.gemini:
        isAvailable = true;
        break;
      case AIProvider.ollama:
        isAvailable = true;
        break;
      case AIProvider.openai:
        isAvailable = _aiSelector.openaiAvailable;
        break;
      case AIProvider.localOllama:
        isAvailable = _aiSelector.localOllamaAvailable;
        break;
    }
    
    if (!isAvailable) {
      debugPrint('   ‚ùå Proveedor $provider no disponible');
      return;
    }

    if (_needsHistoryLoad && _currentProvider != provider) {
      debugPrint('   üìö Detectado cambio de proveedor con historial pendiente');
      debugPrint('   üîÑ Cargando historial en el nuevo proveedor: $provider');
      
      final oldProvider = _currentProvider;
      _currentProvider = provider;
      
      _loadHistoryIntoAIService(_messages);
      _needsHistoryLoad = false;
      
      debugPrint('   ‚úÖ Historial transferido de $oldProvider a $provider');
    }
    
    _currentProvider = provider;
    await _aiSelector.setProvider(provider);
    _updateCommandProcessor();
    
    await _preferencesService.saveLastProvider(provider);
    
    hideModelSelector();
    notifyListeners();
    
    debugPrint('   ‚úÖ Proveedor cambiado a: $provider');
  }

  Future<void> selectOpenAIModel(String modelId) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo OpenAI a: $modelId');

    try {
      await _aiSelector.setOpenAIModel(modelId);
      debugPrint('   ‚úÖ Modelo OpenAI cambiado a: $modelId');
    } catch (e) {
      debugPrint('   ‚ùå Error al cambiar modelo OpenAI: $e');
    }

    notifyListeners();
  }

  Future<bool> retryOllamaConnection() async {
    debugPrint('üîÑ [ChatProvider] Intentando reconectar con Ollama...');
    _isRetryingOllama = true;
    notifyListeners();

    try {
      await _ollamaService.reconnect();
      
      const int maxAttempts = 10;
      const Duration interval = Duration(milliseconds: 300);
      int attempts = 0;
      while (!_aiSelector.ollamaAvailable && attempts < maxAttempts) {
        await Future.delayed(interval);
        attempts++;
      }

      final bool isSuccess = _aiSelector.ollamaAvailable;

      if (isSuccess) {
        _ollamaSelectable = true;
        debugPrint('   üîì Selecci√≥n de Ollama desbloqueada (reconectado)');
        debugPrint('   ‚úÖ [ChatProvider] Reconexi√≥n exitosa. Seleccionando Ollama.');
        await selectProvider(AIProvider.ollama); 
      } else {
        _ollamaSelectable = false;
        debugPrint('   ‚ùå [ChatProvider] La reconexi√≥n fall√≥.');
        if (_currentProvider != AIProvider.gemini) {
          await selectProvider(AIProvider.gemini);
        }
      }
      return isSuccess;
    } catch (e) {
      debugPrint('   ‚ùå [ChatProvider] Error durante la reconexi√≥n: $e');
      return false;
    } finally {
      _isRetryingOllama = false;
      notifyListeners();
    }
  }
  
  Future<void> refreshConnection() async {
    await retryOllamaConnection();
  }

  Future<LocalOllamaInitResult?> initializeLocalOllama() async {
    debugPrint(
        'üöÄ [ChatProvider] Iniciando instalaci√≥n/configuraci√≥n de Ollama Embebido...');

    try {
      final result = await _aiSelector.initializeLocalOllama();

      if (result.success) {
        debugPrint('   ‚úÖ Ollama Embebido inicializado correctamente');
        if (_aiSelector.localOllamaAvailable) {
          await selectProvider(AIProvider.localOllama);
        }
        notifyListeners();
      } else {
        debugPrint('   ‚ùå Error en inicializaci√≥n: ${result.error}');
      }

      return result;
    } catch (e) {
      debugPrint('   ‚ùå Excepci√≥n durante inicializaci√≥n: $e');
      return null;
    }
  }

  void _addWelcomeMessage() {
    final welcomeMessage = CommandsHelp.getWelcomeMessage();

    final welcomeEntity = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: welcomeMessage,
      type: MessageTypeEntity.bot,
      timestamp: DateTime.now(),
    );

    _messages.add(welcomeEntity);
    notifyListeners();
  }

  /// Refresca los quick responses, √∫til cuando se crean/editan comandos de usuario
  /// o cuando cambia la preferencia de agrupar comandos del sistema
  Future<void> refreshQuickResponses() async {
    await _updateQuickResponses();
    notifyListeners();
  }

  /// Env√≠o con streaming (para Gemini)
  Future<void> _sendMessageWithStreaming(String content) async {
    debugPrint('\nüåä [ChatProvider] === ENVIANDO CON STREAMING ===');
    debugPrint('   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor: $_currentProvider');

    if (_needsHistoryLoad) {
      _loadHistoryIntoAIService(_messages);
      _needsHistoryLoad = false;
    }

    if (_isNewConversation) _isNewConversation = false;
    hideModelSelector();

    // Generar IDs √∫nicos garantizados
    final now = DateTime.now();
    final userMessageId = '${now.millisecondsSinceEpoch}_user';
    final botMessageId = '${now.millisecondsSinceEpoch}_bot';

    // A√±adir mensaje del usuario
    final userMessageEntity = MessageEntity(
      id: userMessageId,
      content: content,
      type: MessageTypeEntity.user,
      timestamp: now,
    );
    _messages.add(userMessageEntity);
    notifyListeners();

    // Crear mensaje del bot vac√≠o
    _messages.add(MessageEntity(
      id: botMessageId,
      content: '',
      type: MessageTypeEntity.bot,
      timestamp: now,
    ));

    _isProcessing = true;
    _isStreaming = true;
    notifyListeners();

    final buffer = StringBuffer();

    try {
      final adapter = _aiSelector.getCurrentAdapter();
      
      await for (final chunk in adapter.generateContentStream(content)) {
        buffer.write(chunk);
        
        final index = _messages.indexWhere((m) => m.id == botMessageId);
        if (index != -1) {
          _messages[index] = MessageEntity(
            id: botMessageId,
            content: buffer.toString(),
            type: MessageTypeEntity.bot,
            timestamp: now,
          );
          notifyListeners();
        }
      }

      debugPrint('‚úÖ [ChatProvider] Streaming completado: ${buffer.length} caracteres');

    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error en streaming: $e');

      final index = _messages.indexWhere((m) => m.id == botMessageId);
      if (index != -1) {
        _messages[index] = MessageEntity(
          id: botMessageId,
          content: buffer.isNotEmpty 
              ? '${buffer.toString()}\n\n‚ùå Error: $e'
              : '‚ùå Error: $e',
          type: MessageTypeEntity.bot,
          timestamp: now,
        );
      }
    } finally {
      _isProcessing = false;
      _isStreaming = false;
      _hasUnsavedChanges = true;
      await _updateQuickResponses();
      notifyListeners();
    }
  }

  /// Env√≠o de comando con streaming
  Future<void> _sendCommandWithStreaming(String content) async {
    debugPrint('\nüåä [ChatProvider] === ENVIANDO COMANDO CON STREAMING ===');
    debugPrint('   üí¨ Comando: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor: $_currentProvider');

    hideModelSelector();

    // Generar IDs √∫nicos
    final now = DateTime.now();
    final userMessageId = '${now.millisecondsSinceEpoch}_user';
    final botMessageId = '${now.millisecondsSinceEpoch}_bot';

    // A√±adir mensaje del usuario
    final userMessageEntity = MessageEntity(
      id: userMessageId,
      content: content,
      type: MessageTypeEntity.user,
      timestamp: now,
    );
    _messages.add(userMessageEntity);
    notifyListeners();

    // Procesar comando para obtener el stream
    final commandResult = await _commandProcessor.processMessageStream(content);

    if (!commandResult.isCommand) {
      // No deber√≠a pasar, pero por si acaso, usar flujo normal
      debugPrint('   ‚ö†Ô∏è No es comando, redirigiendo a streaming normal');
      return _sendMessageWithStreaming(content);
    }

    // Si hay error de validaci√≥n (ej: falta contenido)
    if (commandResult.error != null) {
      _messages.add(MessageEntity(
        id: botMessageId,
        content: '‚ö†Ô∏è ${commandResult.error}',
        type: MessageTypeEntity.bot,
        timestamp: now,
      ));
      notifyListeners();
      _hasUnsavedChanges = true;
      return;
    }

    // Crear mensaje del bot vac√≠o
    _messages.add(MessageEntity(
      id: botMessageId,
      content: '',
      type: MessageTypeEntity.bot,
      timestamp: now,
    ));

    _isProcessing = true;
    _isStreaming = true;
    notifyListeners();

    final buffer = StringBuffer();

    try {
      await for (final chunk in commandResult.responseStream!) {
        buffer.write(chunk);
        
        final index = _messages.indexWhere((m) => m.id == botMessageId);
        if (index != -1) {
          _messages[index] = MessageEntity(
            id: botMessageId,
            content: buffer.toString(),
            type: MessageTypeEntity.bot,
            timestamp: now,
          );
          notifyListeners();
        }
      }

      debugPrint('‚úÖ [ChatProvider] Comando streaming completado: ${buffer.length} caracteres');

    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error en comando streaming: $e');

      final index = _messages.indexWhere((m) => m.id == botMessageId);
      if (index != -1) {
        _messages[index] = MessageEntity(
          id: botMessageId,
          content: buffer.isNotEmpty 
              ? '${buffer.toString()}\n\n‚ùå Error: $e'
              : '‚ùå Error: $e',
          type: MessageTypeEntity.bot,
          timestamp: now,
        );
      }
    } finally {
      _isProcessing = false;
      _isStreaming = false;
      _hasUnsavedChanges = true;
      await _updateQuickResponses();
      notifyListeners();
    }
  }

  Future<void> sendMessage(String content) async {
  if (content.trim().isEmpty || _isProcessing) return;

  final isCommand = content.trim().startsWith('/');
  
  // Usar streaming para proveedores que lo soportan
  final supportsStreaming = _currentProvider == AIProvider.gemini || 
                            _currentProvider == AIProvider.localOllama || 
                            _currentProvider == AIProvider.ollama;
  
  if (supportsStreaming) {
    if (isCommand) {
      return _sendCommandWithStreaming(content);
    } else {
      return _sendMessageWithStreaming(content);
    }
  }

    debugPrint('\nüöÄ [ChatProvider] === ENVIANDO MENSAJE ===');
    debugPrint(
        '   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor actual: $_currentProvider');

    if (_needsHistoryLoad) {
      debugPrint('   üìö Cargando historial en el proveedor actual antes de enviar...');
      _loadHistoryIntoAIService(_messages);
      _needsHistoryLoad = false;
    }

    // Logs simplificados
    switch (_currentProvider) {
      case AIProvider.ollama:
        debugPrint('   üìù Modelo Ollama (remoto): $_currentModel');
        break;
      case AIProvider.localOllama:
        debugPrint('   üìù Modelo Ollama Local: ${_localOllamaService.currentModel}');
        break;
      case AIProvider.openai:
        debugPrint('   üìù Modelo OpenAI: ${_aiSelector.currentOpenAIModel}');
        break;
      case AIProvider.gemini:
        debugPrint('   üìù Modelo: gemini-2.5-flash');
        break;
    }

    if (_isNewConversation) {
      _isNewConversation = false;
    }

    hideModelSelector();

    final userMessageEntity = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: content,
      type: MessageTypeEntity.user,
      timestamp: DateTime.now(),
    );

    _messages.add(userMessageEntity);
    _isProcessing = true;
    notifyListeners();

    try {
      debugPrint('   üî∏ Procesando mensaje a trav√©s de SendMessageUseCase...');
      // SendMessageUseCase usar√° el CommandProcessor que ya tiene el repositorio inyectado
      final botResponseEntity = await _sendMessageUseCase.execute(content);

      _messages.add(botResponseEntity);
      debugPrint('‚úÖ [ChatProvider] Mensaje procesado exitosamente');
      debugPrint('üü¢ [ChatProvider] === ENV√çO EXITOSO ===\n');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error procesando mensaje: $e');
      debugPrint('üî¥ [ChatProvider] === ENV√çO FALLIDO ===\n');

      String errorMessage = '‚ùå Error: ${e.toString()}';

      if (_currentProvider == AIProvider.ollama) {
        errorMessage += '\n\nüí° El servidor Ollama remoto no est√° disponible.\n'
                       'Cambiando autom√°ticamente a Gemini...';
        
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);

        _currentProvider = AIProvider.gemini;
        _ollamaSelectable = false;
        _updateCommandProcessor();
        await _preferencesService.saveLastProvider(AIProvider.gemini);
        debugPrint('   ‚úÖ [sendMessage catch] CAMBIO AUTOM√ÅTICO a Gemini exitoso');
        
      } else if (_currentProvider == AIProvider.localOllama) {
        errorMessage += '\n\nüí° Ollama Embebido no est√° disponible.\n'
            'Puede que est√© inicializ√°ndose. Espera unos segundos.\n'
            'O prueba con otro proveedor.';
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      } else if (_currentProvider == AIProvider.openai) {
        errorMessage += '\n\nüí° Verifica tu API Key de OpenAI en .env';
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      } else {
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      }
    } finally {
      _isProcessing = false;
      await _updateQuickResponses();
      notifyListeners();

      _hasUnsavedChanges = true;
      debugPrint('üìù [ChatProvider] Cambios pendientes marcados. Se guardar√°n al salir.');
       notifyListeners();
    }
  }

  // ============================================================================
  // _updateQuickResponses con soporte para carpetas y CommandManagementProvider
  // ============================================================================
  Future<void> _updateQuickResponses() async {
    try {
      // Si tenemos CommandManagementProvider, usarlo para obtener datos actualizados
      if (_commandManagementProvider != null) {
      _updateQuickResponsesFromProvider();
      return;
    }
      
      // Fallback: obtener datos directamente del repositorio
      final allCommands = await _commandRepository.getAllCommands();
      final allFolders = await _commandRepository.getAllFolders();
      
      // Obtener preferencia de agrupar comandos del sistema
      final prefs = await SharedPreferences.getInstance();
      final groupSystemCommands = prefs.getBool(_groupSystemCommandsKey) ?? false;
      
      // Usar el m√©todo est√°tico para generar respuestas organizadas
      final organizedResponses = QuickResponseProvider.buildOrganizedResponses(
        commands: allCommands,
        folders: allFolders,
        groupSystemCommands: groupSystemCommands,
      );
      
      _quickResponses = organizedResponses.map((r) => r.toEntity()).toList();
      
      debugPrint('üì¶ [ChatProvider] Quick responses actualizadas (fallback): ${_quickResponses.length} items');
      
    } catch (e) {
      debugPrint('‚ö†Ô∏è [ChatProvider] Error cargando quick responses: $e');
      _quickResponses = QuickResponseProvider.defaultResponsesAsEntities;
    }
  }

  Future<void> _autoSaveConversation() async {
    if (_messages.isEmpty) return;
    try {
      await _conversationRepository.saveConversation(_messages);
      final isSyncEnabled = _getSyncStatus?.call() ?? false;
      if (kDebugMode) {
        debugPrint(
            "üíæ [ChatProvider] Conversaci√≥n guardada autom√°ticamente (${_messages.length} mensajes)");
        if (isSyncEnabled) {
          debugPrint("‚òÅÔ∏è [ChatProvider] Conversaci√≥n sincronizada con la nube");
        }
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar conversaci√≥n: $e');
    }
  }

  Future<void> clearMessages() async {
    debugPrint('üóëÔ∏è [ChatProvider] Limpiando mensajes...');
    
    _messages.clear();
    _isNewConversation = true;
    _needsHistoryLoad = false;
    
    _currentConversationFile = null;
    _hasUnsavedChanges = false;

    _clearAIServiceHistory();
    _addWelcomeMessage();
    notifyListeners();
  }

  void _clearAIServiceHistory() {
    debugPrint('üßπ [ChatProvider] Limpiando historial de servicios de IA...');
    try { _geminiService.clearConversation(); } catch (e) { debugPrint('   ‚ö†Ô∏è Error limpiando Gemini: $e'); }
    try { _openaiService.clearConversation(); } catch (e) { debugPrint('   ‚ö†Ô∏è Error limpiando OpenAI: $e'); }
    try { _ollamaService.clearConversation(); } catch (e) { debugPrint('   ‚ö†Ô∏è Error limpiando Ollama: $e'); }
    try { _localOllamaService.clearConversation(); } catch (e) { debugPrint('   ‚ö†Ô∏è Error limpiando Ollama Local: $e'); }
  }

  Future<void> loadConversation(File file) async {
    if (_currentConversationFile != null && 
        _currentConversationFile!.path == file.path && 
        _hasUnsavedChanges) {
      debugPrint('üõë [ChatProvider] Bloqueada recarga accidental: Ya tienes esta conversaci√≥n abierta con cambios.');
      return;
    }

    debugPrint('üìÇ [ChatProvider] Cargando conversaci√≥n desde archivo...');

    try {
      final loadedMessages = await _conversationRepository.loadConversation(file);
      _messages
        ..clear()
        ..addAll(loadedMessages);

      _isNewConversation = false;
      _needsHistoryLoad = true;
      
      _currentConversationFile = file;
      _hasUnsavedChanges = false;

      await _updateQuickResponses();
      notifyListeners();

      debugPrint('   ‚úÖ Conversaci√≥n cargada (${_messages.length} mensajes)');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cargando conversaci√≥n: $e');
    }
  }

  @override
  void dispose() {
    debugPrint('üî¥ [ChatProvider] Disposing...');
    _commandManagementProvider?.removeListener(_onCommandDataChanged);
    
    _aiSelector.removeListener(_onAiSelectorChanged);
    _aiSelector.dispose();
    super.dispose();
  }

  void _loadHistoryIntoAIService(List<MessageEntity> messages) {
    debugPrint('üìö [ChatProvider] Cargando historial en servicio de IA...');
    debugPrint('   üéØ Proveedor actual: $_currentProvider');
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        _loadGeminiHistory(messages);
        break;
      case AIProvider.openai:
        _loadOpenAIHistory(messages);
        break;
      case AIProvider.ollama:
        _loadOllamaHistory(messages);
        break;
      case AIProvider.localOllama:
        _loadLocalOllamaHistory(messages);
        break;
    }
  }

  void _loadGeminiHistory(List<MessageEntity> messages) {
    try {
      _geminiService.clearConversation();
      for (final message in messages) {
        if (message.type == MessageTypeEntity.user) {
          _geminiService.addUserMessage(message.content);
        } else if (message.type == MessageTypeEntity.bot) {
          _geminiService.addBotMessage(message.content);
        }
      }
      debugPrint('   ‚úÖ Historial de Gemini cargado: ${messages.length} mensajes');
    } catch (e) {
      debugPrint('   ‚ö†Ô∏è Error cargando historial en Gemini: $e');
    }
  }

  void _loadOpenAIHistory(List<MessageEntity> messages) {
    try {
      _openaiService.clearConversation();
      for (final message in messages) {
        if (message.type == MessageTypeEntity.user) {
          _openaiService.addUserMessage(message.content);
        } else if (message.type == MessageTypeEntity.bot) {
          _openaiService.addBotMessage(message.content);
        }
      }
      debugPrint('   ‚úÖ Historial de OpenAI cargado: ${messages.length} mensajes');
    } catch (e) {
      debugPrint('   ‚ö†Ô∏è Error cargando historial en OpenAI: $e');
    }
  }

  void _loadOllamaHistory(List<MessageEntity> messages) {
    try {
      _ollamaService.clearConversation();
      for (final message in messages) {
        if (message.type == MessageTypeEntity.user) {
          _ollamaService.addUserMessage(message.content);
        } else if (message.type == MessageTypeEntity.bot) {
          _ollamaService.addBotMessage(message.content);
        }
      }
      debugPrint('   ‚úÖ Historial de Ollama cargado: ${messages.length} mensajes');
    } catch (e) {
      debugPrint('   ‚ö†Ô∏è Error cargando historial en Ollama: $e');
    }
  }

  void _loadLocalOllamaHistory(List<MessageEntity> messages) {
    try {
      _localOllamaService.clearConversation();
      for (final message in messages) {
        if (message.type == MessageTypeEntity.user) {
          _localOllamaService.addUserMessage(message.content);
        } else if (message.type == MessageTypeEntity.bot) {
          _localOllamaService.addBotMessage(message.content);
        }
      }
      debugPrint('   ‚úÖ Historial de Ollama Local cargado: ${messages.length} mensajes');
    } catch (e) {
      debugPrint('   ‚ö†Ô∏è Error cargando historial en Ollama Local: $e');
    }
  }

  Future<DeleteResult> deleteAllConversations() async {
    try {
      final isSyncEnabled = _getSyncStatus?.call() ?? false;
      await _conversationRepository.deleteAllConversations();
      return DeleteResult(
        success: true,
        syncWasEnabled: isSyncEnabled,
        message: isSyncEnabled 
            ? 'Todas las conversaciones eliminadas (local y nube)'
            : 'Conversaciones eliminadas localmente. Si sincronizaste previamente, permanecen en la nube.',
      );
    } catch (e) {
      return DeleteResult(
        success: false,
        syncWasEnabled: false,
        message: 'Error eliminando conversaciones: $e',
      );
    }
  }

  Future<DeleteResult> deleteConversations(List<File> files) async {
    try {
      final isSyncEnabled = _getSyncStatus?.call() ?? false;
      await _conversationRepository.deleteConversations(files);
      final count = files.length;
      return DeleteResult(
        success: true,
        syncWasEnabled: isSyncEnabled,
        message: isSyncEnabled 
            ? '$count conversaci√≥n(es) eliminada(s) (local y nube)'
            : '$count conversaci√≥n(es) eliminada(s) localmente. Si sincronizaste previamente, permanecen en la nube.',
      );
    } catch (e) {
      return DeleteResult(
        success: false,
        syncWasEnabled: false,
        message: 'Error eliminando conversaciones: $e',
      );
    }
  }

  Future<void> endSession() async {
    if (_isSaving) return; 
    
    if (_messages.isEmpty) return;
    if (_messages.length == 1 && _messages.first.type == MessageTypeEntity.bot) return;
    if (!_hasUnsavedChanges) return;

    _isSaving = true;
    debugPrint('üíæ [ChatProvider] Guardando sesi√≥n (Actualizaci√≥n)...');

    try {
      await _conversationRepository.saveConversation(
        _messages, 
        existingFile: _currentConversationFile
      );
      
      debugPrint('   ‚úÖ Conversaci√≥n guardada/actualizada correctamente.');

    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar sesi√≥n: $e');
    } finally {
      _hasUnsavedChanges = false;
      _isSaving = false;
    }
  }
}

/// Resultado de una operaci√≥n de eliminaci√≥n
class DeleteResult {
  final bool success;
  final bool syncWasEnabled;
  final String message;

  DeleteResult({
    required this.success,
    required this.syncWasEnabled,
    required this.message,
  });
}