import 'package:flutter/foundation.dart';
import 'dart:io';
import '../../data/models/message_model.dart';
import '../../data/models/quick_response_model.dart';
import '../../data/models/ollama_models.dart';
import '../../data/models/ollama_local_models.dart'; // CAMBIADO: nuevo import para Ollama Local
import '../../data/services/gemini_service.dart';
import '../../data/services/ollama_service.dart';
import '../../data/services/openai_service.dart';
import '../../data/services/local_llm_service.dart';
import '../../data/services/ai_service_selector.dart';
import '../../data/services/preferences_service.dart';
import '../../data/services/ai_service_adapters.dart';
import '../../data/repositories/chat_repository.dart';
import '../../data/repositories/conversation_repository.dart';
import '../../domain/usecases/command_processor.dart';
import '../../domain/usecases/send_message_usecase.dart';

class ChatProvider extends ChangeNotifier {
  final List<Message> _messages = [];
  List<QuickResponse> _quickResponses = QuickResponseProvider.defaultResponses;
  bool _isProcessing = false;
  bool _isNewConversation = true;

  late SendMessageUseCase _sendMessageUseCase; // No final - se actualiza al cambiar proveedor
  late final AIServiceSelector _aiSelector;
  late final PreferencesService _preferencesService;
  
  // Referencias a los servicios
  late final GeminiService _geminiService;
  late final OllamaService _ollamaService;
  late final OpenAIService _openaiService;
  late final LocalLLMService _localLLMService;
  
  // Adaptadores
  late final GeminiServiceAdapter _geminiAdapter;
  late OllamaServiceAdapter _ollamaAdapter; // No final porque se recrea
  late final OpenAIServiceAdapter _openaiAdapter;
  late final LocalLLMServiceAdapter _localLLMAdapter;
  
  // CommandProcessor que se actualizar√°
  late CommandProcessor _commandProcessor; // No final - se actualiza al cambiar proveedor
  
  bool _showModelSelector = false;
  List<OllamaModel> _availableModels = [];
  String _currentModel = 'phi3:latest';
  AIProvider _currentProvider = AIProvider.gemini;

  ChatProvider() {
    // Inicializar servicios
    _geminiService = GeminiService();
    _ollamaService = OllamaService();
    _openaiService = OpenAIService();
    _localLLMService = LocalLLMService();
    
    // Crear adaptadores
    _geminiAdapter = GeminiServiceAdapter(_geminiService);
    _ollamaAdapter = OllamaServiceAdapter(_ollamaService, _currentModel);
    _openaiAdapter = OpenAIServiceAdapter(_openaiService);
    _localLLMAdapter = LocalLLMServiceAdapter(_localLLMService);
    
    _preferencesService = PreferencesService();
    
    _aiSelector = AIServiceSelector(
      geminiService: _geminiService,
      ollamaService: _ollamaService,
      openaiService: _openaiService,
      localLLMService: _localLLMService,
    );
    
    // Inicializar CommandProcessor con Gemini por defecto
    _commandProcessor = CommandProcessor(_geminiAdapter);
    final localRepository = LocalChatRepository();

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: localRepository,
    );

    _initializeModels();
    _addWelcomeMessage();
  }

  /// Actualiza el CommandProcessor seg√∫n el proveedor actual
  void _updateCommandProcessor() {
    AIServiceBase currentAdapter;
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        currentAdapter = _geminiAdapter;
        debugPrint('   üîµ Usando GeminiAdapter');
        break;
      case AIProvider.ollama:
        // Actualizar el modelo en el adaptador existente
        _ollamaAdapter.updateModel(_currentModel);
        currentAdapter = _ollamaAdapter;
        debugPrint('   üü™ Usando OllamaAdapter (remoto) con modelo: $_currentModel');
        break;
      case AIProvider.openai:
        currentAdapter = _openaiAdapter;
        debugPrint('   üü¢ Usando OpenAIAdapter');
        break;
      case AIProvider.localLLM:
        currentAdapter = _localLLMAdapter;
        debugPrint('   üü† Usando LocalLLMAdapter (Ollama Local)');
        break;
    }
    
    // Crear nuevo CommandProcessor
    _commandProcessor = CommandProcessor(currentAdapter);
    
    // Actualizar SendMessageUseCase
    final localRepository = LocalChatRepository();
    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: localRepository,
    );
    
    debugPrint('üîÑ [ChatProvider] CommandProcessor actualizado para: $_currentProvider');
  }

  // Getters
  List<Message> get messages => List.unmodifiable(_messages);
  List<QuickResponse> get quickResponses => _quickResponses;
  bool get isProcessing => _isProcessing;
  bool get showModelSelector => _showModelSelector;
  List<OllamaModel> get availableModels => _availableModels;
  String get currentModel => _currentModel;
  AIProvider get currentProvider => _currentProvider;
  ConnectionInfo get connectionInfo => _aiSelector.connectionInfo;
  bool get ollamaAvailable => _aiSelector.ollamaAvailable;
  
  AIServiceSelector get aiSelector => _aiSelector;
  bool get openaiAvailable => _aiSelector.openaiAvailable;
  String get currentOpenAIModel => _aiSelector.currentOpenAIModel;
  List<String> get availableOpenAIModels => _aiSelector.availableOpenAIModels;
  
  // Getters para Ollama Local
  OllamaLocalStatus get localLLMStatus => _aiSelector.localLLMStatus; // CAMBIADO: tipo actualizado
  bool get localLLMAvailable => _aiSelector.localLLMAvailable;
  bool get localLLMLoading => _aiSelector.localLLMLoading;
  String? get localLLMError => _aiSelector.localLLMError;
  LocalLLMService get localLLMService => _aiSelector.localLLMService; // NUEVO: acceso al servicio
  
  Stream<ConnectionInfo> get connectionStream => _aiSelector.connectionStream;

  Future<void> _initializeModels() async {
    try {
      debugPrint('üé¨ [ChatProvider] Inicializando modelos...');
      
      // Los modelos de Ollama ya se inicializan autom√°ticamente en AIServiceSelector
      // Solo necesitamos obtenerlos
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        if (_availableModels.isNotEmpty) {
          _currentModel = _availableModels.first.name;
          _ollamaAdapter.updateModel(_currentModel);
        }
      }
      
      // Restaurar preferencias del usuario
      await _restoreUserPreferences();
      
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error inicializando modelos: $e');
    }
  }
  
  Future<void> _restoreUserPreferences() async {
    try {
      debugPrint('üîÑ [ChatProvider] Restaurando preferencias...');
      
      final lastProvider = await _preferencesService.getLastProvider();
      
      if (lastProvider != null) {
        bool canRestore = false;
        
        switch (lastProvider) {
          case AIProvider.gemini:
            canRestore = true;
            break;
            
          case AIProvider.ollama:
            canRestore = _aiSelector.ollamaAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è Ollama (remoto) no disponible, usando Gemini por defecto');
            }
            break;
            
          case AIProvider.openai:
            canRestore = _aiSelector.openaiAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è OpenAI no disponible, usando Gemini por defecto');
            }
            break;
            
          case AIProvider.localLLM:
            canRestore = _aiSelector.localLLMAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è Ollama Local no disponible, usando Gemini por defecto');
            }
            break;
        }
        
        if (canRestore) {
          _currentProvider = lastProvider;
          await _aiSelector.setProvider(lastProvider);
          _updateCommandProcessor();
          debugPrint('   ‚úÖ Restaurado proveedor: $lastProvider');
          
          await _restoreProviderModel(lastProvider);
        } else {
          _currentProvider = AIProvider.gemini;
          await _aiSelector.setProvider(AIProvider.gemini);
          _updateCommandProcessor();
          debugPrint('   ‚ÑπÔ∏è Usando Gemini por defecto');
        }
      } else {
        debugPrint('   ‚ÑπÔ∏è No hay preferencias previas, usando Gemini');
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error restaurando preferencias: $e');
      _currentProvider = AIProvider.gemini;
      _updateCommandProcessor();
    }
  }
  
  Future<void> _restoreProviderModel(AIProvider provider) async {
    try {
      switch (provider) {
        case AIProvider.ollama:
          final lastModel = await _preferencesService.getLastOllamaModel();
          if (lastModel != null && _availableModels.any((m) => m.name == lastModel)) {
            _currentModel = lastModel;
            _ollamaAdapter.updateModel(_currentModel);
            debugPrint('   ‚úÖ Restaurado modelo Ollama: $lastModel');
          }
          break;
          
        case AIProvider.openai:
          final lastModel = await _preferencesService.getLastOpenAIModel();
          if (lastModel != null) {
            _aiSelector.setOpenAIModel(lastModel);
            debugPrint('   ‚úÖ Restaurado modelo OpenAI: $lastModel');
          }
          break;
          
        case AIProvider.localLLM:
          // Ollama Local no necesita restaurar modelo espec√≠fico
          // El servicio usa el modelo que est√© configurado en Ollama
          debugPrint('   ‚ÑπÔ∏è Ollama Local usar√° el modelo disponible');
          break;
          
        case AIProvider.gemini:
          // Gemini no tiene selecci√≥n de modelo
          break;
      }
    } catch (e) {
      debugPrint('‚ö†Ô∏è [ChatProvider] Error restaurando modelo: $e');
    }
  }

  void toggleModelSelector() {
    _showModelSelector = !_showModelSelector;
    notifyListeners();
  }

  void hideModelSelector() {
    if (_showModelSelector) {
      _showModelSelector = false;
      notifyListeners();
    }
  }

  Future<void> changeProvider(AIProvider newProvider) async {
    debugPrint('üîÑ [ChatProvider] Cambiando proveedor a: $newProvider');
    
    try {
      await _aiSelector.setProvider(newProvider);
      _currentProvider = newProvider;
      _updateCommandProcessor();
      
      await _preferencesService.saveLastProvider(newProvider);
      
      notifyListeners();
      debugPrint('‚úÖ [ChatProvider] Proveedor cambiado exitosamente');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando proveedor: $e');
      rethrow;
    }
  }

  Future<void> changeModel(String modelName) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo Ollama a: $modelName');
    
    try {
      await _aiSelector.setOllamaModel(modelName);
      _currentModel = modelName;
      _ollamaAdapter.updateModel(modelName);
      _updateCommandProcessor();
      
      await _preferencesService.saveLastOllamaModel(modelName);
      
      notifyListeners();
      debugPrint('‚úÖ [ChatProvider] Modelo cambiado exitosamente');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo: $e');
      rethrow;
    }
  }

  Future<void> changeOpenAIModel(String modelName) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo OpenAI a: $modelName');
    
    try {
      _aiSelector.setOpenAIModel(modelName);
      
      await _preferencesService.saveLastOpenAIModel(modelName);
      
      notifyListeners();
      debugPrint('‚úÖ [ChatProvider] Modelo OpenAI cambiado exitosamente');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo OpenAI: $e');
      rethrow;
    }
  }

  Future<void> refreshModels() async {
    debugPrint('üîÑ [ChatProvider] Refrescando modelos...');
    
    try {
      // Refrescar Ollama usando el m√©todo p√∫blico
      await _aiSelector.refreshOllama();
      
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        
        // Si el modelo actual ya no est√° disponible, cambiar al primero
        if (_availableModels.isNotEmpty && 
            !_availableModels.any((m) => m.name == _currentModel)) {
          _currentModel = _availableModels.first.name;
          _ollamaAdapter.updateModel(_currentModel);
          
          if (_currentProvider == AIProvider.ollama) {
            _updateCommandProcessor();
          }
          debugPrint('   ‚ÑπÔ∏è Modelo cambiado a: $_currentModel');
        }
      }
      
      debugPrint('   ‚úÖ Modelos refrescados');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error refrescando modelos: $e');
    }
  }

  // MODIFICADO: M√©todos para Ollama Local con nuevo tipo de retorno
  Future<OllamaLocalInitResult> initializeLocalLLM() async {
    debugPrint('üöÄ [ChatProvider] Iniciando Ollama Local...');
    try {
      final result = await _aiSelector.initializeLocalLLM();
      notifyListeners();
      
      if (result.success) {
        debugPrint('‚úÖ [ChatProvider] Ollama Local inicializado correctamente');
        debugPrint('   üì¶ Modelo: ${result.modelName}');
        debugPrint('   üìã Modelos disponibles: ${result.availableModels?.join(", ")}');
      } else {
        debugPrint('‚ùå [ChatProvider] Error inicializando Ollama Local: ${result.error}');
      }
      
      return result;
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Excepci√≥n al inicializar Ollama Local: $e');
      notifyListeners();
      return OllamaLocalInitResult(
        success: false,
        error: 'Error inesperado: $e',
      );
    }
  }

  Future<void> stopLocalLLM() async {
    debugPrint('üõë [ChatProvider] Deteniendo Ollama Local...');
    try {
      await _aiSelector.stopLocalLLM();
      notifyListeners();
      debugPrint('‚úÖ [ChatProvider] Ollama Local detenido correctamente');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error deteniendo Ollama Local: $e');
      notifyListeners();
    }
  }

  Future<OllamaLocalInitResult> retryLocalLLM() async {
    debugPrint('üîÑ [ChatProvider] Reintentando inicializaci√≥n de Ollama Local...');
    return await initializeLocalLLM();
  }

  void _addWelcomeMessage() {
    final welcomeMessage = '''
¬°Bienvenido al chat! üëã

Aqu√≠ puedes conversar conmigo y utilizar los siguientes comandos:

**Comandos disponibles:**

- **/tryprompt** [escribe aqu√≠ tu prompt] -- Este comando te permite ejecutar un an√°lisis y mejora de tu prompt, generando como resultado un prompt mejorado en caso de que sea posible.

üí° **Nuevo:** Los comandos ahora usan la IA que tengas seleccionada. Cambia entre:
‚Ä¢ **Gemini** - R√°pido y gratis
‚Ä¢ **ChatGPT** - Alta calidad (requiere API Key)
‚Ä¢ **Ollama Remoto** - Servidor Ubuntu con Phi3/Mistral
‚Ä¢ **Ollama Local** - 100% privado en tu PC

¬°Empieza escribiendo tu mensaje!''';

    _messages.add(Message.bot(welcomeMessage));
    notifyListeners();
  }

  Future<void> sendMessage(String content) async {
    if (content.trim().isEmpty || _isProcessing) return;

    debugPrint('\nüöÄ [ChatProvider] === ENVIANDO MENSAJE ===');
    debugPrint('   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor actual: $_currentProvider');
    
    // Log del modelo seg√∫n el proveedor
    switch (_currentProvider) {
      case AIProvider.ollama:
        debugPrint('   üìù Modelo Ollama (remoto): $_currentModel');
        break;
      case AIProvider.localLLM:
        debugPrint('   üìù Modelo Ollama Local: ${_localLLMService.currentModel}');
        break;
      case AIProvider.openai:
        debugPrint('   üìù Modelo OpenAI: ${_aiSelector.currentOpenAIModel}');
        break;
      case AIProvider.gemini:
        debugPrint('   üìù Modelo: gemini-2.5-flash');
        break;
    }

    if (_isNewConversation) {
      _isNewConversation = false;
    }

    hideModelSelector();

    final userMessage = Message.user(content);
    _messages.add(userMessage);
    _isProcessing = true;
    notifyListeners();

    try {
      String botResponse;
      
      // Los comandos se procesan seg√∫n el proveedor actual
      if (content.startsWith('/')) {
        debugPrint('   üî∏ Detectado comando, procesando con $_currentProvider');
        final response = await _sendMessageUseCase.execute(content);
        botResponse = response.content;
      } 
      // Caso especial: Ollama remoto con historial
      else if (_currentProvider == AIProvider.ollama && _aiSelector.ollamaAvailable) {
        debugPrint('   üü™ Usando Ollama (servidor remoto)...');
        botResponse = await _sendToOllama(content);
      }
      // Caso especial: Ollama Local con historial
      else if (_currentProvider == AIProvider.localLLM && _aiSelector.localLLMAvailable) {
        debugPrint('   üü† Usando Ollama Local...');
        botResponse = await _sendToLocalLLM(content);
      }
      // Resto de proveedores (Gemini, OpenAI)
      else {
        debugPrint('   üü¶ Usando $_currentProvider...');
        final response = await _sendMessageUseCase.execute(content);
        botResponse = response.content;
      }
      
      _messages.add(Message.bot(botResponse));
      debugPrint('‚úÖ [ChatProvider] Mensaje procesado exitosamente');
      debugPrint('üü¢ [ChatProvider] === ENV√çO EXITOSO ===\n');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error procesando mensaje: $e');
      debugPrint('üî¥ [ChatProvider] === ENV√çO FALLIDO ===\n');
      
      String errorMessage = '‚ùå Error: ${e.toString()}';
      
      // Mensajes de ayuda contextuales seg√∫n el proveedor
      if (_currentProvider == AIProvider.ollama) {
        errorMessage += '\n\nüí° El servidor Ollama remoto no est√° disponible.\n'
                       '¬øQuieres probar con otro proveedor? Toca el selector arriba.';
      } else if (_currentProvider == AIProvider.localLLM) {
        errorMessage += '\n\nüí° Ollama Local no est√° disponible.\n'
                       'Verifica que Ollama est√© ejecut√°ndose: ollama serve\n'
                       'O prueba con otro proveedor.';
      } else if (_currentProvider == AIProvider.openai) {
        errorMessage += '\n\nüí° Verifica tu API Key de OpenAI en .env';
      }
      
      _messages.add(Message.bot(errorMessage));
    } finally {
      _isProcessing = false;
      _updateQuickResponses();
      notifyListeners();

      await _autoSaveConversation();
    }
  }

  // Enviar a Ollama (servidor remoto) con historial
  Future<String> _sendToOllama(String content) async {
    try {
      debugPrint('   üì§ [ChatProvider] Preparando mensaje para Ollama (remoto)...');
      debugPrint('   üéØ Modelo: $_currentModel');
      
      final response = await _aiSelector.sendMessage(
        content,
        history: _messages.where((m) => !m.content.contains('¬°Bienvenido al chat!')).toList(),
      );
      
      debugPrint('   ‚úÖ Respuesta recibida de Ollama (${response.length} caracteres)');
      return response;
    } catch (e) {
      debugPrint('   ‚ùå Error con Ollama: $e');
      throw Exception('Error con Ollama remoto: $e');
    }
  }

  // NUEVO: Enviar a Ollama Local con historial
  Future<String> _sendToLocalLLM(String content) async {
    try {
      debugPrint('   üì§ [ChatProvider] Preparando mensaje para Ollama Local...');
      debugPrint('   üéØ Modelo: ${_localLLMService.currentModel}');
      
      final response = await _aiSelector.sendMessage(
        content,
        history: _messages.where((m) => !m.content.contains('¬°Bienvenido al chat!')).toList(),
      );
      
      debugPrint('   ‚úÖ Respuesta recibida de Ollama Local (${response.length} caracteres)');
      return response;
    } catch (e) {
      debugPrint('   ‚ùå Error con Ollama Local: $e');
      throw Exception('Error con Ollama Local: $e');
    }
  }

  // Helper para convertir historial (usado por Ollama remoto)
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];

    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;

    for (final msg in recentHistory) {
      if (msg.content.contains('¬°Bienvenido al chat!')) continue;
      
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.content,
      ));
    }

    return messages;
  }

  void _updateQuickResponses() {
    _quickResponses = QuickResponseProvider.getContextualResponses(_messages);
  }

  Future<void> _autoSaveConversation() async {
    if (_messages.isEmpty) return;
    try {
      await ConversationRepository.saveConversation(_messages);
      if (kDebugMode) {
        debugPrint("üíæ [ChatProvider] Conversaci√≥n guardada autom√°ticamente (${_messages.length} mensajes)");
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar conversaci√≥n: $e');
    }
  }

  Future<void> clearMessages({bool saveBeforeClear = true}) async {
    debugPrint('üóëÔ∏è [ChatProvider] Limpiando mensajes...');
    
    if (saveBeforeClear && _messages.isNotEmpty) {
      await ConversationRepository.saveConversation(_messages);
    }
    _messages.clear();
    _isNewConversation = true;
    
    _addWelcomeMessage();
    notifyListeners();
    
    debugPrint('   ‚úÖ Mensajes limpiados');
  }

  Future<void> loadConversation(File file) async {
    debugPrint('üìÇ [ChatProvider] Cargando conversaci√≥n desde archivo...');
    
    final loadedMessages = await ConversationRepository.loadConversation(file);
    _messages
      ..clear()
      ..addAll(loadedMessages);
    
    _isNewConversation = false;
    
    _updateQuickResponses();
    notifyListeners();
    
    debugPrint('   ‚úÖ Conversaci√≥n cargada (${_messages.length} mensajes)');
  }

  @override
  void dispose() {
    debugPrint('üî¥ [ChatProvider] Disposing...');
    _aiSelector.dispose();
    super.dispose();
  }
}