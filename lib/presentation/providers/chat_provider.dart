import 'package:flutter/foundation.dart';
import 'dart:io';
import '../../data/models/message_model.dart';
import '../../data/models/quick_response_model.dart';
import '../../data/models/ollama_models.dart';
import '../../data/models/local_llm_models.dart';
import '../../data/services/gemini_service.dart';
import '../../data/services/ollama_service.dart';
import '../../data/services/openai_service.dart';
import '../../data/services/local_llm_service.dart';
import '../../data/services/ai_service_selector.dart';
import '../../data/services/preferences_service.dart';
import '../../data/services/ai_service_adapters.dart';
import '../../data/repositories/chat_repository.dart';
import '../../data/repositories/conversation_repository.dart';
import '../../domain/usecases/command_processor.dart';
import '../../domain/usecases/send_message_usecase.dart';

class ChatProvider extends ChangeNotifier {
  final List<Message> _messages = [];
  List<QuickResponse> _quickResponses = QuickResponseProvider.defaultResponses;
  bool _isProcessing = false;
  bool _isNewConversation = true;

  late SendMessageUseCase _sendMessageUseCase; // No final - se actualiza al cambiar proveedor
  late final AIServiceSelector _aiSelector;
  late final PreferencesService _preferencesService;
  
  // Referencias a los servicios
  late final GeminiService _geminiService;
  late final OllamaService _ollamaService;
  late final OpenAIService _openaiService;
  late final LocalLLMService _localLLMService;
  
  // Adaptadores
  late final GeminiServiceAdapter _geminiAdapter;
  late OllamaServiceAdapter _ollamaAdapter; // No final porque se recrea
  late final OpenAIServiceAdapter _openaiAdapter;
  late final LocalLLMServiceAdapter _localLLMAdapter;
  
  // CommandProcessor que se actualizar√°
  late CommandProcessor _commandProcessor; // No final - se actualiza al cambiar proveedor
  
  bool _showModelSelector = false;
  List<OllamaModel> _availableModels = [];
  String _currentModel = 'phi3:latest';
  AIProvider _currentProvider = AIProvider.gemini;

  ChatProvider() {
    // Inicializar servicios
    _geminiService = GeminiService();
    _ollamaService = OllamaService();
    _openaiService = OpenAIService();
    _localLLMService = LocalLLMService();
    
    // Crear adaptadores
    _geminiAdapter = GeminiServiceAdapter(_geminiService);
    _ollamaAdapter = OllamaServiceAdapter(_ollamaService, _currentModel);
    _openaiAdapter = OpenAIServiceAdapter(_openaiService);
    _localLLMAdapter = LocalLLMServiceAdapter(_localLLMService);
    
    _preferencesService = PreferencesService();
    
    _aiSelector = AIServiceSelector(
      geminiService: _geminiService,
      ollamaService: _ollamaService,
      openaiService: _openaiService,
      localLLMService: _localLLMService,
    );
    
    // Inicializar CommandProcessor con Gemini por defecto
    _commandProcessor = CommandProcessor(_geminiAdapter);
    final localRepository = LocalChatRepository();

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: localRepository,
    );

    _initializeModels();
    _addWelcomeMessage();
  }

  /// Actualiza el CommandProcessor seg√∫n el proveedor actual
  void _updateCommandProcessor() {
    AIServiceBase currentAdapter;
    
    switch (_currentProvider) {
      case AIProvider.gemini:
        currentAdapter = _geminiAdapter;
        debugPrint('   üîµ Usando GeminiAdapter');
        break;
      case AIProvider.ollama:
        // Actualizar el modelo en el adaptador existente
        _ollamaAdapter.updateModel(_currentModel);
        currentAdapter = _ollamaAdapter;
        debugPrint('   üü™ Usando OllamaAdapter con modelo: $_currentModel');
        break;
      case AIProvider.openai:
        currentAdapter = _openaiAdapter;
        debugPrint('   üü¢ Usando OpenAIAdapter');
        break;
      case AIProvider.localLLM:
        currentAdapter = _localLLMAdapter;
        debugPrint('   üü† Usando LocalLLMAdapter');
        break;
    }
    
    // Crear nuevo CommandProcessor
    _commandProcessor = CommandProcessor(currentAdapter);
    
    // Actualizar SendMessageUseCase
    final localRepository = LocalChatRepository();
    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: localRepository,
    );
    
    debugPrint('üîÑ [ChatProvider] CommandProcessor actualizado para: $_currentProvider');
  }

  // Getters
  List<Message> get messages => List.unmodifiable(_messages);
  List<QuickResponse> get quickResponses => _quickResponses;
  bool get isProcessing => _isProcessing;
  bool get showModelSelector => _showModelSelector;
  List<OllamaModel> get availableModels => _availableModels;
  String get currentModel => _currentModel;
  AIProvider get currentProvider => _currentProvider;
  ConnectionInfo get connectionInfo => _aiSelector.connectionInfo;
  bool get ollamaAvailable => _aiSelector.ollamaAvailable;
  
  AIServiceSelector get aiSelector => _aiSelector;
  bool get openaiAvailable => _aiSelector.openaiAvailable;
  String get currentOpenAIModel => _aiSelector.currentOpenAIModel;
  List<String> get availableOpenAIModels => _aiSelector.availableOpenAIModels;
  
  LocalLLMStatus get localLLMStatus => _aiSelector.localLLMStatus;
  bool get localLLMAvailable => _aiSelector.localLLMAvailable;
  bool get localLLMLoading => _aiSelector.localLLMLoading;
  String? get localLLMError => _aiSelector.localLLMError;
  
  Stream<ConnectionInfo> get connectionStream => _aiSelector.connectionStream;

  Future<void> _initializeModels() async {
    try {
      debugPrint('üé¨ [ChatProvider] Inicializando modelos...');
      
      await _aiSelector.refreshOllama();
      
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        if (_availableModels.isNotEmpty) {
          _currentModel = _availableModels.first.name;
          _ollamaAdapter.updateModel(_currentModel);
        }
      }
      
      await _restoreUserPreferences();
      
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error inicializando modelos: $e');
    }
  }
  
  Future<void> _restoreUserPreferences() async {
    try {
      debugPrint('üîÑ [ChatProvider] Restaurando preferencias...');
      
      final lastProvider = await _preferencesService.getLastProvider();
      
      if (lastProvider != null) {
        bool canRestore = false;
        
        switch (lastProvider) {
          case AIProvider.gemini:
            canRestore = true;
            break;
            
          case AIProvider.ollama:
            canRestore = _aiSelector.ollamaAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è Ollama no disponible, usando Gemini por defecto');
            }
            break;
            
          case AIProvider.openai:
            canRestore = _aiSelector.openaiAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è OpenAI no disponible, usando Gemini por defecto');
            }
            break;
            
          case AIProvider.localLLM:
            canRestore = _aiSelector.localLLMAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è LLM Local no disponible, usando Gemini por defecto');
            }
            break;
        }
        
        if (canRestore) {
          _currentProvider = lastProvider;
          await _aiSelector.setProvider(lastProvider);
          _updateCommandProcessor();
          debugPrint('   ‚úÖ Restaurado proveedor: $lastProvider');
          
          await _restoreProviderModel(lastProvider);
        } else {
          _currentProvider = AIProvider.gemini;
          await _aiSelector.setProvider(AIProvider.gemini);
          _updateCommandProcessor();
          debugPrint('   ‚ÑπÔ∏è Usando Gemini por defecto');
        }
      } else {
        debugPrint('   ‚ÑπÔ∏è No hay preferencias previas, usando Gemini');
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error restaurando preferencias: $e');
      _currentProvider = AIProvider.gemini;
      _updateCommandProcessor();
    }
  }
  
  Future<void> _restoreProviderModel(AIProvider provider) async {
    try {
      switch (provider) {
        case AIProvider.ollama:
          final lastModel = await _preferencesService.getLastOllamaModel();
          if (lastModel != null && _availableModels.any((m) => m.name == lastModel)) {
            _currentModel = lastModel;
            await _aiSelector.setOllamaModel(lastModel);
            _ollamaAdapter.updateModel(lastModel);
            debugPrint('   ‚úÖ Restaurado modelo Ollama: $lastModel');
          } else {
            debugPrint('   ‚ÑπÔ∏è Modelo Ollama no disponible, usando: $_currentModel');
          }
          break;
          
        case AIProvider.openai:
          final lastModel = await _preferencesService.getLastOpenAIModel();
          if (lastModel != null && _aiSelector.availableOpenAIModels.contains(lastModel)) {
            _aiSelector.setOpenAIModel(lastModel);
            debugPrint('   ‚úÖ Restaurado modelo OpenAI: $lastModel');
          } else {
            debugPrint('   ‚ÑπÔ∏è Usando modelo OpenAI por defecto');
          }
          break;
          
        case AIProvider.gemini:
        case AIProvider.localLLM:
          break;
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error restaurando modelo: $e');
    }
  }

  void toggleModelSelector() {
    _showModelSelector = !_showModelSelector;
    notifyListeners();
  }

  void hideModelSelector() {
    if (_showModelSelector) {
      _showModelSelector = false;
      notifyListeners();
    }
  }

  Future<void> changeProvider(AIProvider provider) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando proveedor a: $provider');
      
      // Validaci√≥n especial para LocalLLM
      if (provider == AIProvider.localLLM) {
        debugPrint('   üîç Verificando estado del LLM local...');
        if (_aiSelector.localLLMStatus != LocalLLMStatus.ready) {
          final statusText = _aiSelector.localLLMStatus.displayText;
          debugPrint('   ‚ùå LLM local no est√° listo: $statusText');
          throw Exception('El modelo local no est√° listo ($statusText). In√≠cialo primero.');
        }
        debugPrint('   ‚úÖ LLM local est√° listo');
      }
      
      if (provider == AIProvider.ollama && !_aiSelector.ollamaAvailable) {
        throw Exception('Ollama no est√° disponible');
      }
      
      if (provider == AIProvider.openai && !_aiSelector.openaiAvailable) {
        throw Exception('OpenAI no est√° disponible. Configura tu API key.');
      }
      
      // Cambiar proveedor en el selector
      await _aiSelector.setProvider(provider).timeout(
        const Duration(seconds: 10),
        onTimeout: () {
          throw Exception('Timeout al cambiar proveedor. El servicio no responde.');
        },
      );
      
      _currentProvider = provider;
      
      // Actualizar CommandProcessor
      _updateCommandProcessor();
      
      await _preferencesService.saveLastProvider(provider);
      
      debugPrint('   ‚úÖ Proveedor cambiado a: $provider');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando proveedor: $e');
      
      // Mensaje m√°s espec√≠fico seg√∫n el error
      String errorMessage = e.toString();
      if (errorMessage.contains('Timeout')) {
        errorMessage = 'El servicio no responde. Intenta nuevamente.';
      } else if (errorMessage.contains('LateInitialization')) {
        errorMessage = 'Error interno al cambiar proveedor. Reinicia la app.';
      }
      
      throw Exception(errorMessage);
    }
  }

  /// Cambiar modelo de Ollama (nombre correcto del m√©todo)
  Future<void> changeModel(String modelName) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando modelo Ollama a: $modelName');
      
      if (!_aiSelector.ollamaAvailable) {
        throw Exception('Ollama no est√° disponible');
      }
      
      await _aiSelector.setOllamaModel(modelName);
      _currentModel = modelName;
      
      // Si Ollama est√° activo, actualizar el adaptador
      if (_currentProvider == AIProvider.ollama) {
        _ollamaAdapter.updateModel(modelName);
        debugPrint('   üîÑ Adaptador actualizado con modelo: $modelName');
      }
      
      await _preferencesService.saveLastOllamaModel(modelName);
      
      debugPrint('   ‚úÖ Modelo cambiado a: $modelName');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo: $e');
      rethrow;
    }
  }

  Future<void> changeOpenAIModel(String modelName) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando modelo OpenAI a: $modelName');
      
      if (!_aiSelector.openaiAvailable) {
        throw Exception('OpenAI no est√° disponible');
      }
      
      _aiSelector.setOpenAIModel(modelName);
      
      await _preferencesService.saveLastOpenAIModel(modelName);
      
      debugPrint('   ‚úÖ Modelo OpenAI cambiado a: $modelName');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo OpenAI: $e');
      rethrow;
    }
  }

  /// Refrescar modelos de Ollama (nombre correcto del m√©todo)
  Future<void> refreshModels() async {
    try {
      debugPrint('üîÑ [ChatProvider] Refrescando modelos de Ollama...');
      
      await _aiSelector.refreshOllama();
      
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        
        if (_availableModels.isNotEmpty && 
            !_availableModels.any((m) => m.name == _currentModel)) {
          _currentModel = _availableModels.first.name;
          await _aiSelector.setOllamaModel(_currentModel);
          if (_currentProvider == AIProvider.ollama) {
            _ollamaAdapter.updateModel(_currentModel);
          }
          debugPrint('   ‚ÑπÔ∏è Modelo cambiado a: $_currentModel');
        }
      }
      
      debugPrint('   ‚úÖ Modelos refrescados');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error refrescando modelos: $e');
    }
  }

  Future<LocalLLMInitResult> initializeLocalLLM() async {
    debugPrint('üöÄ [ChatProvider] Iniciando LLM local...');
    try {
      final result = await _aiSelector.initializeLocalLLM();
      notifyListeners();
      
      if (result.success) {
        debugPrint('‚úÖ [ChatProvider] LLM local inicializado correctamente');
        debugPrint('   üì¶ Modelo: ${result.modelName}');
        debugPrint('   üíæ Tama√±o: ${result.modelSize}');
      } else {
        debugPrint('‚ùå [ChatProvider] Error inicializando LLM local: ${result.error}');
      }
      
      return result;
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Excepci√≥n al inicializar LLM local: $e');
      notifyListeners();
      return LocalLLMInitResult(
        success: false,
        error: 'Error inesperado: $e',
      );
    }
  }

  Future<void> stopLocalLLM() async {
    debugPrint('üõë [ChatProvider] Deteniendo LLM local...');
    try {
      await _aiSelector.stopLocalLLM();
      notifyListeners();
      debugPrint('‚úÖ [ChatProvider] LLM local detenido correctamente');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error deteniendo LLM local: $e');
      notifyListeners();
    }
  }

  Future<LocalLLMInitResult> retryLocalLLM() async {
    debugPrint('üîÑ [ChatProvider] Reintentando inicializaci√≥n del LLM local...');
    return await initializeLocalLLM();
  }

  void _addWelcomeMessage() {
    final welcomeMessage = '''
¬°Bienvenido al chat! üëã

Aqu√≠ puedes conversar conmigo y utilizar los siguientes comandos:

**Comandos disponibles:**

- **/tryprompt** [escribe aqu√≠ tu prompt] -- Este comando te permite ejecutar un an√°lisis y mejora de tu prompt, generando como resultado un prompt mejorado en caso de que sea posible.

üí° **Nuevo:** Los comandos ahora usan la IA que tengas seleccionada. Cambia entre Gemini, OpenAI, Ollama o IA Local usando el selector arriba.

¬°Empieza escribiendo tu mensaje!''';

    _messages.add(Message.bot(welcomeMessage));
    notifyListeners();
  }

  Future<void> sendMessage(String content) async {
    if (content.trim().isEmpty || _isProcessing) return;

    debugPrint('\nüöÄ [ChatProvider] === ENVIANDO MENSAJE ===');
    debugPrint('   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor actual: $_currentProvider');
    debugPrint('   üìù Modelo actual: $_currentModel');

    if (_isNewConversation) {
      _isNewConversation = false;
    }

    hideModelSelector();

    final userMessage = Message.user(content);
    _messages.add(userMessage);
    _isProcessing = true;
    notifyListeners();

    try {
      String botResponse;
      
      // Los comandos ahora se procesan seg√∫n el proveedor actual
      if (content.startsWith('/')) {
        debugPrint('   üî∏ Detectado comando, procesando con $_currentProvider');
        final response = await _sendMessageUseCase.execute(content);
        botResponse = response.content;
      } else if (_currentProvider == AIProvider.ollama && _aiSelector.ollamaAvailable) {
        debugPrint('   üü™ Usando Ollama...');
        botResponse = await _sendToOllama(content);
      } else {
        debugPrint('   üü¶ Usando $_currentProvider...');
        final response = await _sendMessageUseCase.execute(content);
        botResponse = response.content;
      }
      
      _messages.add(Message.bot(botResponse));
      debugPrint('‚úÖ [ChatProvider] Mensaje procesado exitosamente');
      debugPrint('üü¢ [ChatProvider] === ENV√çO EXITOSO ===\n');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error procesando mensaje: $e');
      debugPrint('üî¥ [ChatProvider] === ENV√çO FALLIDO ===\n');
      
      String errorMessage = '‚ùå Error inesperado: ${e.toString()}';
      
      if (_currentProvider == AIProvider.ollama) {
        errorMessage += '\n\nüí° ¬øQuieres probar con Gemini? Toca el selector de modelos arriba.';
      }
      
      _messages.add(Message.bot(errorMessage));
    } finally {
      _isProcessing = false;
      _updateQuickResponses();
      notifyListeners();

      await _autoSaveConversation();
    }
  }

  Future<String> _sendToOllama(String content) async {
    try {
      debugPrint('   üì§ [ChatProvider] Preparando mensaje para Ollama...');
      debugPrint('   üéØ Modelo: $_currentModel');
      
      debugPrint('   üí¨ Enviando mensaje normal a Ollama');
      
      final response = await _aiSelector.sendMessage(
        content,
        history: _messages.where((m) => !m.content.contains('¬°Bienvenido al chat!')).toList(),
      );
      
      debugPrint('   ‚úÖ Respuesta recibida de Ollama (${response.length} caracteres)');
      return response;
    } catch (e) {
      debugPrint('   ‚ùå Error con Ollama: $e');
      throw Exception('Error con Ollama: $e');
    }
  }

  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];

    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;

    for (final msg in recentHistory) {
      if (msg.content.contains('¬°Bienvenido al chat!')) continue;
      
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.content,
      ));
    }

    return messages;
  }

  void _updateQuickResponses() {
    _quickResponses = QuickResponseProvider.getContextualResponses(_messages);
  }

  Future<void> _autoSaveConversation() async {
    if (_messages.isEmpty) return;
    try {
      await ConversationRepository.saveConversation(_messages);
      if (kDebugMode) {
        debugPrint("üíæ [ChatProvider] Conversaci√≥n guardada autom√°ticamente (${_messages.length} mensajes)");
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar conversaci√≥n: $e');
    }
  }

  Future<void> clearMessages({bool saveBeforeClear = true}) async {
    debugPrint('üóëÔ∏è [ChatProvider] Limpiando mensajes...');
    
    if (saveBeforeClear && _messages.isNotEmpty) {
      await ConversationRepository.saveConversation(_messages);
    }
    _messages.clear();
    _isNewConversation = true;
    
    _addWelcomeMessage();
    notifyListeners();
    
    debugPrint('   ‚úÖ Mensajes limpiados');
  }

  Future<void> loadConversation(File file) async {
    debugPrint('üìÇ [ChatProvider] Cargando conversaci√≥n desde archivo...');
    
    final loadedMessages = await ConversationRepository.loadConversation(file);
    _messages
      ..clear()
      ..addAll(loadedMessages);
    
    _isNewConversation = false;
    
    _updateQuickResponses();
    notifyListeners();
    
    debugPrint('   ‚úÖ Conversaci√≥n cargada (${_messages.length} mensajes)');
  }

  @override
  void dispose() {
    debugPrint('üî¥ [ChatProvider] Disposing...');
    _aiSelector.dispose();
    super.dispose();
  }
}