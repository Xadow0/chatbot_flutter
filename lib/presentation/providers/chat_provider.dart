import 'package:flutter/foundation.dart';
import 'dart:io';
import '../../domain/entities/message_entity.dart';
import '../../domain/entities/quick_response_entity.dart';
import '../../data/models/message_model.dart';
import '../../data/models/quick_response_model.dart';
import '../../data/models/remote_ollama_models.dart';
import '../../data/models/local_ollama_models.dart';
import '../../data/services/gemini_service.dart';
import '../../data/services/ollama_service.dart';
import '../../data/services/openai_service.dart';
import '../../data/services/local_ollama_service.dart';
import '../../data/services/ai_service_selector.dart';
import '../../data/services/preferences_service.dart';
import '../../data/services/ai_service_adapters.dart';
import '../../domain/usecases/command_processor.dart';
import '../../domain/usecases/send_message_usecase.dart';
import '../../domain/repositories/chat_repository.dart';
import '../../domain/repositories/conversation_repository.dart';
import '../../core/constants/commands_help.dart';

/// Provider principal del chat
///
/// IMPORTANTE: Este provider trabaja INTERNAMENTE con ENTIDADES (domain layer).
/// Solo convierte a modelos cuando necesita interactuar con servicios de persistencia.
class ChatProvider extends ChangeNotifier {
  // ============================================================================
  // ESTADO INTERNO: ENTIDADES (Domain Layer)
  // ============================================================================
  final List<MessageEntity> _messages = [];
  List<QuickResponseEntity> _quickResponses =
      QuickResponseProvider.defaultResponsesAsEntities;
  bool _isProcessing = false;
  bool _isNewConversation = true;
  bool _isRetryingOllama = false;
  // Controla si el usuario puede seleccionar Ollama remoto desde la UI.
  // Esto se usa para bloquear la selecci√≥n temporalmente cuando el servidor
  // remoto se desconecta y la app cambia autom√°ticamente a Gemini.
  bool _ollamaSelectable = true;

  late SendMessageUseCase _sendMessageUseCase; // No final - se actualiza al cambiar proveedor
  late final AIServiceSelector _aiSelector;
  late final PreferencesService _preferencesService;

  // INTERFACES DE REPOSITORIO
  final ChatRepository _chatRepository;
  final ConversationRepository _conversationRepository;

  // Referencias a los servicios
  late final GeminiService _geminiService;
  late final OllamaService _ollamaService;
  late final OpenAIService _openaiService;
  late final OllamaManagedService _localOllamaService;

  // Adaptadores
  late final GeminiServiceAdapter _geminiAdapter;
  late OllamaServiceAdapter _ollamaAdapter; // No final porque se recrea
  late final OpenAIServiceAdapter _openaiAdapter;
  late final LocalOllamaServiceAdapter _localOllamaAdapter;

  // CommandProcessor que se actualizar√°
  late CommandProcessor _commandProcessor; // No final - se actualiza al cambiar proveedor

  bool _showModelSelector = false;
  List<OllamaModel> _availableModels = [];
  String _currentModel = 'phi3:latest';
  AIProvider _currentProvider = AIProvider.gemini;

  ChatProvider({
    required ChatRepository chatRepository,
    required ConversationRepository conversationRepository,
  })  : _chatRepository = chatRepository,
        _conversationRepository = conversationRepository {
    // Inicializar servicios
    _geminiService = GeminiService();
    _ollamaService = OllamaService();
    _openaiService = OpenAIService();
    _localOllamaService = OllamaManagedService();

    // Crear adaptadores
    _geminiAdapter = GeminiServiceAdapter(_geminiService);
    _ollamaAdapter = OllamaServiceAdapter(_ollamaService, _currentModel);
    _openaiAdapter = OpenAIServiceAdapter(_openaiService);
    _localOllamaAdapter = LocalOllamaServiceAdapter(_localOllamaService);

    _preferencesService = PreferencesService();

    _aiSelector = AIServiceSelector(
      geminiService: _geminiService,
      ollamaService: _ollamaService,
      openaiService: _openaiService,
      localOllamaService: _localOllamaService,
    );

    // Suscribirse a los cambios del selector
    _aiSelector.addListener(_onAiSelectorChanged);

    // Inicializar CommandProcessor con Gemini por defecto
    _commandProcessor = CommandProcessor(_geminiAdapter);

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: _chatRepository, // <- Inyectar aqu√≠
    );

    // Inicializar modelos y agregar mensaje de bienvenida
    _initializeModels().then((_) => _addWelcomeMessage());
  }

  // ===================================================================
  // ‚ñº‚ñº‚ñº MODIFICACI√ìN 1: Cambiar la firma a 'async' ‚ñº‚ñº‚ñº
  // ===================================================================
  /// Escucha los cambios de AIServiceSelector y notifica a los listeners de ChatProvider
  Future<void> _onAiSelectorChanged() async {
    debugPrint('üîÑ [ChatProvider] AIServiceSelector notific√≥ cambios, actualizando UI...');

    // 1. Sincronizar la lista de modelos disponibles
    if (_aiSelector.ollamaAvailable) {
      // Si el selector indica que Ollama est√° disponible, asegurarnos
      // de que la UI pueda seleccionarlo (desbloquear selecci√≥n).
      if (!_ollamaSelectable) {
        _ollamaSelectable = true;
        debugPrint('   üîì Ollama disponible: desbloqueando selecci√≥n en la UI');
      }
      // Comprobar si la lista de modelos ha cambiado realmente (optimizaci√≥n)
      // usar√° 'listEquals' de foundation.dart (que ya est√° importado)
      if (!listEquals(_availableModels, _aiSelector.availableModels)) {
        _availableModels = _aiSelector.availableModels;
        debugPrint(
            '   ‚úÖ Lista de modelos Ollama (remoto) actualizada: ${_availableModels.length} modelos');

        // 2. Si la lista se actualiz√≥, asegurarse de que haya un modelo seleccionado
        if (_availableModels.isNotEmpty) {
          final currentModelExists =
              _availableModels.any((m) => m.name == _currentModel);

          // Si el modelo actual no existe (o no hab√≠a ninguno),
          // seleccionar el primero de la lista.
          if (!currentModelExists || _currentModel.isEmpty) {
            _currentModel = _availableModels.first.name;
            _ollamaAdapter.updateModel(_currentModel);
            debugPrint(
                '   ‚ö†Ô∏è Modelo actual no encontrado. Seleccionando por defecto: $_currentModel');
          }
        }
      }

      // 3. Sincronizar el modelo actual
      if (_currentProvider == AIProvider.ollama &&
          _currentModel != _aiSelector.currentOllamaModel) {
        _currentModel = _aiSelector.currentOllamaModel;
        _ollamaAdapter.updateModel(_currentModel);
      }
    } else {
      // Ollama (remoto) NO est√° disponible

      // 4. Si Ollama se desconecta, vaciar la lista
      if (_availableModels.isNotEmpty) {
        _availableModels = [];
        debugPrint('   ‚ùå Ollama (remoto) desconectado. Vaciando lista de modelos.');
      }

      // Bloquear la posibilidad de seleccionar Ollama desde la UI hasta
      // que el usuario vuelva a reconectar manualmente o el servicio
      // sea detectado como disponible de nuevo.
      if (_ollamaSelectable) {
        _ollamaSelectable = false;
        debugPrint('   üîí Bloqueando selecci√≥n de Ollama en la UI (desconectado)');
      }

      // ===================================================================
      // ‚ñº‚ñº‚ñº MODIFICACI√ìN 2: Usar 'await' para el cambio de proveedor ‚ñº‚ñº‚ñº
      // ===================================================================

      // 5. VERIFICAR SI OLLAMA (Remoto) ERA EL PROVEEDOR ACTIVO
      if (_currentProvider == AIProvider.ollama) {
        debugPrint('   ‚ö†Ô∏è ¬°Ollama (remoto) era el proveedor activo y se ha desconectado!');
        debugPrint('   üîÑ Cambiando autom√°ticamente a Gemini por defecto...');

        // 6. Esperar a que el cambio de proveedor se complete
        await selectProvider(AIProvider.gemini);

        debugPrint('   ‚úÖ [AIServiceSelector change] Cambio a Gemini completado.');
        
        // 7. A√±adir un mensaje al chat DESPU√âS de que el cambio se haya hecho
        _addOllamaConnectionErrorMessage();
        
        // El notifyListeners() del final se encargar√° de actualizar la UI
        // con el proveedor ya cambiado y el mensaje nuevo.
      }
      // ===================================================================
      // ‚ñ≤‚ñ≤‚ñ≤ FIN DE LA MODIFICACI√ìN ‚ñ≤‚ñ≤‚ñ≤
      // ===================================================================
    }

    // Notificar a la UI (ModelSelectorBubble) para que se reconstruya
    // Con la l√≥gica 'await' de arriba, esta notificaci√≥n es AHORA
    // 100% segura y reflejar√° el estado correcto.
    notifyListeners();
  }

  // ===================================================================
  // ‚ñº‚ñº‚ñº NUEVO M√âTODO A√ëADIDO (de la vez anterior, sin cambios) ‚ñº‚ñº‚ñº
  // ===================================================================

  /// A√±ade un mensaje de error al chat cuando Ollama (remoto) se desconecta
  void _addOllamaConnectionErrorMessage() {
    final errorMessage = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: '‚ùå El servidor de Ollama (remoto) se ha desconectado.\n\n'
          'Se ha cambiado autom√°ticamente a **Gemini**.\n\n'
          'Por favor, comprueba la conexi√≥n del servidor e int√©ntalo de nuevo m√°s tarde.',
      type: MessageTypeEntity.bot,
      timestamp: DateTime.now(),
    );

    // A√±adir solo si no es el √∫ltimo mensaje (evitar duplicados)
    if (_messages.isEmpty ||
        (_messages.last.type != MessageTypeEntity.bot) ||
        (_messages.last.content != errorMessage.content)) {
      _messages.add(errorMessage);
    }
  }

  /// Actualiza el CommandProcessor seg√∫n el proveedor actual
  void _updateCommandProcessor() {
    AIServiceBase currentAdapter;

    switch (_currentProvider) {
      case AIProvider.gemini:
        currentAdapter = _geminiAdapter;
        debugPrint('   üîµ Usando GeminiAdapter');
        break;
      case AIProvider.ollama:
        // Actualizar el modelo en el adaptador existente
        _ollamaAdapter.updateModel(_currentModel);
        currentAdapter = _ollamaAdapter;
        debugPrint(
            '   üü™ Usando OllamaAdapter (remoto) con modelo: $_currentModel');
        break;
      case AIProvider.openai:
        currentAdapter = _openaiAdapter;
        debugPrint('   üü¢ Usando OpenAIAdapter');
        break;
      case AIProvider.localOllama:
        currentAdapter = _localOllamaAdapter;
        debugPrint('   üü† Usando LocalLLMAdapter (Ollama Embebido)');
        break;
    }

    // Crear nuevo CommandProcessor
    _commandProcessor = CommandProcessor(currentAdapter);

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: _commandProcessor,
      chatRepository: _chatRepository,
    );

    debugPrint(
        'üîÑ [ChatProvider] CommandProcessor actualizado para: $_currentProvider');
  }

  // ============================================================================
  // GETTERS: EXPONE ENTIDADES A LA UI
  // ============================================================================
  List<MessageEntity> get messages => List.unmodifiable(_messages);
  List<QuickResponseEntity> get quickResponses => _quickResponses;
  bool get isProcessing => _isProcessing;
  bool get showModelSelector => _showModelSelector;
  List<OllamaModel> get availableModels => _availableModels;
  String get currentModel => _currentModel;
  AIProvider get currentProvider => _currentProvider;
  ConnectionInfo get connectionInfo => _aiSelector.connectionInfo;
  // Exponer a la UI s√≥lo si el service selector reporta disponibilidad
  // y no hemos bloqueado la selecci√≥n tras un cambio autom√°tico.
  bool get ollamaAvailable => _aiSelector.ollamaAvailable && _ollamaSelectable;
  bool get isRetryingOllama => _isRetryingOllama;

  AIServiceSelector get aiSelector => _aiSelector;
  bool get openaiAvailable => _aiSelector.openaiAvailable;
  String get currentOpenAIModel => _aiSelector.currentOpenAIModel;
  List<String> get availableOpenAIModels => _aiSelector.availableOpenAIModels;

  // Getters para Ollama Local
  LocalOllamaStatus get localOllamaStatus => _aiSelector.localOllamaStatus;
  bool get localOllamaAvailable => _aiSelector.localOllamaAvailable;
  bool get localOllamaLoading => _aiSelector.localOllamaLoading;

  Stream<ConnectionInfo> get connectionStream => _aiSelector.connectionStream;

  // ============================================================================
  // M√âTODOS PARA GESTI√ìN DE HISTORIAL (para HistoryPage)
  // ============================================================================

  /// Expone el m√©todo listConversations del repositorio a la UI
  Future<List<FileSystemEntity>> listConversations() {
    return _conversationRepository.listConversations();
  }

  /// Expone el m√©todo deleteAllConversations del repositorio a la UI
  Future<void> deleteAllConversations() async {
    await _conversationRepository.deleteAllConversations();
    notifyListeners(); // Notificar si la UI debe reaccionar
  }

  Future<void> deleteConversations(List<File> files) async {
    await _conversationRepository.deleteConversations(files);
    notifyListeners();
  }

  Future<void> _initializeModels() async {
    try {
      debugPrint('üé¨ [ChatProvider] Inicializando modelos...');

      // Los modelos de Ollama ya se inicializan autom√°ticamente en AIServiceSelector
      // Solo necesitamos obtenerlos
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        if (_availableModels.isNotEmpty) {
          _currentModel = _availableModels.first.name;
          _ollamaAdapter.updateModel(_currentModel);
        }
      }

      // Restaurar preferencias del usuario
      await _restoreUserPreferences();

  // Asegurar el estado inicial de selecci√≥n de Ollama
  _ollamaSelectable = _aiSelector.ollamaAvailable;

      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error inicializando modelos: $e');
    }
  }

  Future<void> _restoreUserPreferences() async {
    try {
      debugPrint('üîÑ [ChatProvider] Restaurando preferencias...');

      final lastProvider = await _preferencesService.getLastProvider();

      if (lastProvider != null) {
        bool canRestore = false;

        switch (lastProvider) {
          case AIProvider.gemini:
            canRestore = true;
            break;

          case AIProvider.ollama:
            canRestore = _aiSelector.ollamaAvailable;
            if (!canRestore) {
              debugPrint(
                  '   ‚ö†Ô∏è Ollama (remoto) no disponible, usando Gemini por defecto');
            }
            break;

          case AIProvider.openai:
            canRestore = _aiSelector.openaiAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è OpenAI no disponible, usando Gemini por defecto');
            }
            break;

          case AIProvider.localOllama:
            canRestore = _aiSelector.localOllamaAvailable;
            if (!canRestore) {
              debugPrint(
                  '   ‚ö†Ô∏è Ollama Embebido no disponible, usando Gemini por defecto');
            }
            break;
        }

        if (canRestore) {
          _currentProvider = lastProvider;
          _updateCommandProcessor();
          debugPrint('   ‚úÖ Proveedor restaurado: $lastProvider');
        }
      }
    } catch (e) {
      debugPrint('   ‚ùå Error restaurando preferencias: $e');
    }
  }

  void toggleModelSelector() {
    debugPrint(
        'üîÑ [ChatProvider] Toggling model selector: $_showModelSelector -> ${!_showModelSelector}');
    _showModelSelector = !_showModelSelector;
    notifyListeners();
  }

  void hideModelSelector() {
    if (_showModelSelector) {
      debugPrint('üîÑ [ChatProvider] Hiding model selector');
      _showModelSelector = false;
      notifyListeners();
    }
  }

  Future<void> selectModel(String modelName) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo a: $modelName');

    try {
      _currentModel = modelName;
      _aiSelector.setOllamaModel(modelName);
      _ollamaAdapter.updateModel(modelName);
      hideModelSelector();
      notifyListeners();
      debugPrint('   ‚úÖ Modelo cambiado a: $modelName');
    } catch (e) {
      debugPrint('   ‚ùå Error al cambiar modelo: $e');
      // Revertir el cambio local
      _currentModel = _aiSelector.currentOllamaModel;
      notifyListeners();
    }
  }

  Future<void> selectProvider(AIProvider provider) async {
    debugPrint('üîÑ [ChatProvider] Cambiando proveedor a: $provider');

    // ===================================================================
    // ‚ñº‚ñº‚ñº L√ìGICA DE SELECCI√ìN MODIFICADA ‚ñº‚ñº‚ñº
    // ===================================================================
    
    // Si el proveedor es Ollama y no est√° disponible, NO cambiar a Gemini
    // autom√°ticamente aqu√≠. El usuario debe reintentar.
    // El cambio autom√°tico a Gemini S√ìLO ocurre pasivamente 
    // (en _onAiSelectorChanged) o activamente (en sendMessage).
    if (provider == AIProvider.ollama && !_aiSelector.ollamaAvailable) {
      debugPrint('   ‚ùå Ollama (remoto) no disponible. No se puede seleccionar por ahora.');
      // No cambiamos el proveedor ni marcamos la tarjeta como seleccionada.
      // En su lugar devolvemos el control para que el UI muestre el estado
      // como no disponible (igual que al iniciar la app sin conexi√≥n).
      return;
    }
    
    // Verificar disponibilidad de otros proveedores
    bool isAvailable = false;
    switch (provider) {
      case AIProvider.gemini:
        isAvailable = true;
        break;
      case AIProvider.ollama: // Ya sabemos que est√° disponible por el check de arriba
        isAvailable = true;
        break;
      case AIProvider.openai:
        isAvailable = _aiSelector.openaiAvailable;
        break;
      case AIProvider.localOllama:
        isAvailable = _aiSelector.localOllamaAvailable;
        break;
    }
    
    if (!isAvailable) {
      debugPrint('   ‚ùå Proveedor $provider no disponible');
      return;
    }
    
    // ===================================================================
    // ‚ñ≤‚ñ≤‚ñ≤ FIN DE LA MODIFICACI√ìN ‚ñ≤‚ñ≤‚ñ≤
    // ===================================================================

    _currentProvider = provider;
    _aiSelector.setProvider(provider);
    _updateCommandProcessor();
    
    // Guardar preferencia
    await _preferencesService.saveLastProvider(provider);
    
    hideModelSelector();
    notifyListeners();
    
    debugPrint('   ‚úÖ Proveedor cambiado a: $provider');
  }

  Future<void> selectOpenAIModel(String modelId) async {
    debugPrint('üîÑ [ChatProvider] Cambiando modelo OpenAI a: $modelId');

    try {
      await _aiSelector.setOpenAIModel(modelId);
      debugPrint('   ‚úÖ Modelo OpenAI cambiado a: $modelId');
    } catch (e) {
      debugPrint('   ‚ùå Error al cambiar modelo OpenAI: $e');
    }

    notifyListeners();
  }

  Future<bool> retryOllamaConnection() async {
    debugPrint('üîÑ [ChatProvider] Intentando reconectar con Ollama...');
    _isRetryingOllama = true;
    notifyListeners();

    try {
      // Llama al 'reconnect' del servicio.
      // Este m√©todo (en OllamaService) llama a _detectBestConnection()
      // lo cual actualiza el stream _connectionController.
      await _ollamaService.reconnect();
      
      // El stream habr√° notificado al AIServiceSelector,
      // y el AIServiceSelector habr√° notificado a este ChatProvider
      // (via _onAiSelectorChanged).
      
      // Por lo tanto, _aiSelector.ollamaAvailable ya estar√° actualizado.
      // Es posible que la actualizaci√≥n del AIServiceSelector sea as√≠ncrona y
      // a√∫n no se haya reflejado inmediatamente tras el reconnect().
      // Esperamos un breve periodo (poll) para que el selector procese el
      // nuevo estado antes de considerar la reconexi√≥n fallida.
      const int maxAttempts = 10; // poll attempts
      const Duration interval = Duration(milliseconds: 300);
      int attempts = 0;
      while (!_aiSelector.ollamaAvailable && attempts < maxAttempts) {
        await Future.delayed(interval);
        attempts++;
      }

      final bool isSuccess = _aiSelector.ollamaAvailable;

      if (isSuccess) {
        // Permitir seleccionar Ollama ahora que est√° disponible
        _ollamaSelectable = true;
        debugPrint('   üîì Selecci√≥n de Ollama desbloqueada (reconectado)');
        debugPrint('   ‚úÖ [ChatProvider] Reconexi√≥n exitosa. Seleccionando Ollama.');
        // Si tiene √©xito, seleccionar activamente Ollama
        await selectProvider(AIProvider.ollama); 
      } else {
        // Mantener bloqueada la selecci√≥n si la reconexi√≥n no tuvo √©xito
        _ollamaSelectable = false;
        debugPrint('   ‚ùå [ChatProvider] La reconexi√≥n fall√≥.');
        // Si falla, asegurarse de que estamos en Gemini
        if (_currentProvider != AIProvider.gemini) {
          await selectProvider(AIProvider.gemini);
        }
      }
      return isSuccess;
    } catch (e) {
      debugPrint('   ‚ùå [ChatProvider] Error durante la reconexi√≥n: $e');
      return false;
    } finally {
      _isRetryingOllama = false;
      notifyListeners();
    }
  }
  
  // Este m√©todo es para el "pull-to-refresh" o similar,
  // ahora solo redirige al nuevo.
  Future<void> refreshConnection() async {
    await retryOllamaConnection();
  }

  Future<LocalOllamaInitResult?> initializeLocalOllama() async {
    debugPrint(
        'üöÄ [ChatProvider] Iniciando instalaci√≥n/configuraci√≥n de Ollama Embebido...');

    try {
      final result = await _aiSelector.initializeLocalOllama();

      if (result.success) {
        debugPrint('   ‚úÖ Ollama Embebido inicializado correctamente');

        // Auto-cambiar a Local Ollama si tuvo √©xito
        if (_aiSelector.localOllamaAvailable) {
          await selectProvider(AIProvider.localOllama);
        }

        notifyListeners();
      } else {
        debugPrint('   ‚ùå Error en inicializaci√≥n: ${result.error}');
      }

      return result;
    } catch (e) {
      debugPrint('   ‚ùå Excepci√≥n durante inicializaci√≥n: $e');
      return null;
    }
  }

  void _addWelcomeMessage() {
    // Obtener el mensaje de bienvenida completo desde CommandsHelp
    final welcomeMessage = CommandsHelp.getWelcomeMessage();

    final welcomeEntity = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: welcomeMessage,
      type: MessageTypeEntity.bot,
      timestamp: DateTime.now(),
    );

    _messages.add(welcomeEntity);
    notifyListeners();
  }

  Future<void> sendMessage(String content) async {
    if (content.trim().isEmpty || _isProcessing) return;

    debugPrint('\nüöÄ [ChatProvider] === ENVIANDO MENSAJE ===');
    debugPrint(
        '   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor actual: $_currentProvider');

    // Log del modelo seg√∫n el proveedor
    switch (_currentProvider) {
      case AIProvider.ollama:
        debugPrint('   üìù Modelo Ollama (remoto): $_currentModel');
        break;
      case AIProvider.localOllama:
        debugPrint('   üìù Modelo Ollama Local: ${_localOllamaService.currentModel}');
        break;
      case AIProvider.openai:
        debugPrint('   üìù Modelo OpenAI: ${_aiSelector.currentOpenAIModel}');
        break;
      case AIProvider.gemini:
        debugPrint('   üìù Modelo: gemini-2.5-flash');
        break;
    }

    if (_isNewConversation) {
      _isNewConversation = false;
    }

    hideModelSelector();

    // Crear entidad de mensaje del usuario
    final userMessageEntity = MessageEntity(
      id: DateTime.now().millisecondsSinceEpoch.toString(),
      content: content,
      type: MessageTypeEntity.user,
      timestamp: DateTime.now(),
    );

    _messages.add(userMessageEntity);
    _isProcessing = true;
    notifyListeners();

    try {
      // SendMessageUseCase trabaja con entidades y retorna una entidad
      debugPrint('   üî∏ Procesando mensaje a trav√©s de SendMessageUseCase...');
      final botResponseEntity = await _sendMessageUseCase.execute(content);

      _messages.add(botResponseEntity);
      debugPrint('‚úÖ [ChatProvider] Mensaje procesado exitosamente');
      debugPrint('üü¢ [ChatProvider] === ENV√çO EXITOSO ===\n');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error procesando mensaje: $e');
      debugPrint('üî¥ [ChatProvider] === ENV√çO FALLIDO ===\n');

      String errorMessage = '‚ùå Error: ${e.toString()}';

      // ===================================================================
      // ‚ñº‚ñº‚ñº L√ìGICA DE ERROR DE ENV√çO MODIFICADA ‚ñº‚ñº‚ñº
      // ===================================================================
      if (_currentProvider == AIProvider.ollama) {
        errorMessage += '\n\nüí° El servidor Ollama remoto no est√° disponible.\n'
                       'Cambiando autom√°ticamente a Gemini...';
        
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);

        // Forzar el cambio de proveedor a Gemini
        // NO llamamos a selectProvider, sino que cambiamos el estado
        // internamente para que la UI se actualice.
        _currentProvider = AIProvider.gemini;
  // Marcar Ollama como no seleccionable tras el fallo/auto-cambio
  _ollamaSelectable = false;
        _updateCommandProcessor();
        await _preferencesService.saveLastProvider(AIProvider.gemini);
        debugPrint('   ‚úÖ [sendMessage catch] CAMBIO AUTOM√ÅTICO a Gemini exitoso');
        
      } else if (_currentProvider == AIProvider.localOllama) {
        errorMessage += '\n\nüí° Ollama Embebido no est√° disponible.\n'
            'Puede que est√© inicializ√°ndose. Espera unos segundos.\n'
            'O prueba con otro proveedor.';
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      } else if (_currentProvider == AIProvider.openai) {
        errorMessage += '\n\nüí° Verifica tu API Key de OpenAI en .env';
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      } else {
        // Error gen√©rico o de Gemini
        final errorEntity = MessageEntity(
          id: DateTime.now().millisecondsSinceEpoch.toString(),
          content: errorMessage,
          type: MessageTypeEntity.bot,
          timestamp: DateTime.now(),
        );
        _messages.add(errorEntity);
      }
      
      // ===================================================================
      // ‚ñ≤‚ñ≤‚ñ≤ FIN DE LA SECCI√ìN DE ERROR ‚ñ≤‚ñ≤‚ñ≤
      // ===================================================================

    } finally {
      _isProcessing = false;
      _updateQuickResponses();
      notifyListeners();

      await _autoSaveConversation();
    }
  }

  void _updateQuickResponses() {
    // Convertir entidades a modelos solo para obtener respuestas contextuales
    final messageModels =
        _messages.map((entity) => Message.fromEntity(entity)).toList();
    _quickResponses =
        QuickResponseProvider.getContextualResponsesAsEntities(messageModels);
  }

  Future<void> _autoSaveConversation() async {
    if (_messages.isEmpty) return;
    try {
      // ConversationRepository trabaja con entidades
      await _conversationRepository.saveConversation(_messages);
      if (kDebugMode) {
        debugPrint(
            "üíæ [ChatProvider] Conversaci√≥n guardada autom√°ticamente (${_messages.length} mensajes)");
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar conversaci√≥n: $e');
    }
  }

  Future<void> clearMessages({bool saveBeforeClear = true}) async {
    debugPrint('üóëÔ∏è [ChatProvider] Limpiando mensajes...');

    if (saveBeforeClear && _messages.isNotEmpty) {
      await _conversationRepository.saveConversation(_messages);
    }
    _messages.clear();
    _isNewConversation = true;

    _addWelcomeMessage();
    notifyListeners();

    debugPrint('   ‚úÖ Mensajes limpiados');
  }

  Future<void> loadConversation(File file) async {
    debugPrint('üìÇ [ChatProvider] Cargando conversaci√≥n desde archivo...');

    // ConversationRepository retorna entidades
    final loadedMessages =
        await _conversationRepository.loadConversation(file);
    _messages
      ..clear()
      ..addAll(loadedMessages);

    _isNewConversation = false;

    _updateQuickResponses();
    notifyListeners();

    debugPrint('   ‚úÖ Conversaci√≥n cargada (${_messages.length} mensajes)');
  }

  @override
  void dispose() {
    debugPrint('üî¥ [ChatProvider] Disposing...');
    _aiSelector.removeListener(_onAiSelectorChanged);

    _aiSelector.dispose();
    super.dispose();
  }
}