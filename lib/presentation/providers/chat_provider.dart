import 'package:flutter/foundation.dart';
import 'dart:io';
import '../../data/models/message_model.dart';
import '../../data/models/quick_response_model.dart';
import '../../data/models/ollama_models.dart';
import '../../data/services/gemini_service.dart';
import '../../data/services/ollama_service.dart';
import '../../data/services/openai_service.dart';
import '../../data/services/ai_service_selector.dart';
import '../../data/services/preferences_service.dart';
import '../../data/repositories/chat_repository.dart';
import '../../data/repositories/conversation_repository.dart';
import '../../domain/usecases/command_processor.dart';
import '../../domain/usecases/send_message_usecase.dart';

class ChatProvider extends ChangeNotifier {
  final List<Message> _messages = [];
  List<QuickResponse> _quickResponses = QuickResponseProvider.defaultResponses;
  bool _isProcessing = false;
  bool _isNewConversation = true;

  late final SendMessageUseCase _sendMessageUseCase;
  late final AIServiceSelector _aiSelector;
  late final PreferencesService _preferencesService;
  
  // Estado espec√≠fico para la selecci√≥n de modelos
  bool _showModelSelector = false;
  List<OllamaModel> _availableModels = [];
  String _currentModel = 'phi3:latest';
  AIProvider _currentProvider = AIProvider.gemini;

  ChatProvider() {
    final geminiService = GeminiService();
    final ollamaService = OllamaService();
    final openaiService = OpenAIService();
    
    // Inicializar servicio de preferencias
    _preferencesService = PreferencesService();
    
    // Inicializar selector de IA
    _aiSelector = AIServiceSelector(
      geminiService: geminiService,
      ollamaService: ollamaService,
      openaiService: openaiService,
    );
    
    // Configurar command processor con ambos servicios
    final commandProcessor = CommandProcessor(geminiService);
    final localRepository = LocalChatRepository();

    _sendMessageUseCase = SendMessageUseCase(
      commandProcessor: commandProcessor,
      chatRepository: localRepository,
    );

    // Inicializar modelos disponibles y restaurar preferencias
    _initializeModels();
    
    // A√±adir mensaje de bienvenida al iniciar una conversaci√≥n nueva
    _addWelcomeMessage();
  }

  // Getters
  List<Message> get messages => List.unmodifiable(_messages);
  List<QuickResponse> get quickResponses => _quickResponses;
  bool get isProcessing => _isProcessing;
  bool get showModelSelector => _showModelSelector;
  List<OllamaModel> get availableModels => _availableModels;
  String get currentModel => _currentModel;
  AIProvider get currentProvider => _currentProvider;
  ConnectionInfo get connectionInfo => _aiSelector.connectionInfo;
  bool get ollamaAvailable => _aiSelector.ollamaAvailable;
  
  // Nuevos getters para OpenAI
  AIServiceSelector get aiSelector => _aiSelector;
  bool get openaiAvailable => _aiSelector.openaiAvailable;
  String get currentOpenAIModel => _aiSelector.currentOpenAIModel;
  List<String> get availableOpenAIModels => _aiSelector.availableOpenAIModels;
  
  // Stream de conexi√≥n
  Stream<ConnectionInfo> get connectionStream => _aiSelector.connectionStream;

  /// Inicializar modelos disponibles y restaurar preferencias
  Future<void> _initializeModels() async {
    try {
      debugPrint('üé¨ [ChatProvider] Inicializando modelos...');
      
      // Verificar si Ollama est√° disponible
      await _aiSelector.refreshOllama();
      
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        if (_availableModels.isNotEmpty) {
          _currentModel = _availableModels.first.name;
        }
      }
      
      // Restaurar preferencias del usuario
      await _restoreUserPreferences();
      
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error inicializando modelos: $e');
    }
  }
  
  /// Restaurar las preferencias del usuario
  Future<void> _restoreUserPreferences() async {
    try {
      debugPrint('üîÑ [ChatProvider] Restaurando preferencias...');
      
      // Obtener √∫ltimo proveedor usado
      final lastProvider = await _preferencesService.getLastProvider();
      
      if (lastProvider != null) {
        // Verificar disponibilidad del proveedor antes de restaurarlo
        bool canRestore = false;
        
        switch (lastProvider) {
          case AIProvider.gemini:
            // Gemini siempre disponible
            canRestore = true;
            break;
            
          case AIProvider.ollama:
            // Verificar si Ollama est√° disponible
            canRestore = _aiSelector.ollamaAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è Ollama no disponible, usando Gemini por defecto');
            }
            break;
            
          case AIProvider.openai:
            // Verificar si OpenAI est√° configurado
            canRestore = _aiSelector.openaiAvailable;
            if (!canRestore) {
              debugPrint('   ‚ö†Ô∏è OpenAI no disponible, usando Gemini por defecto');
            }
            break;
        }
        
        if (canRestore) {
          _currentProvider = lastProvider;
          await _aiSelector.setProvider(lastProvider);
          debugPrint('   ‚úÖ Restaurado proveedor: $lastProvider');
          
          // Restaurar modelo espec√≠fico seg√∫n el proveedor
          await _restoreProviderModel(lastProvider);
        } else {
          // Si no se puede restaurar, usar Gemini por defecto
          _currentProvider = AIProvider.gemini;
          await _aiSelector.setProvider(AIProvider.gemini);
          debugPrint('   ‚ÑπÔ∏è Usando Gemini por defecto');
        }
      } else {
        debugPrint('   ‚ÑπÔ∏è No hay preferencias previas, usando Gemini');
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error restaurando preferencias: $e');
      // En caso de error, usar Gemini por defecto
      _currentProvider = AIProvider.gemini;
    }
  }
  
  /// Restaurar el modelo espec√≠fico del proveedor
  Future<void> _restoreProviderModel(AIProvider provider) async {
    try {
      switch (provider) {
        case AIProvider.ollama:
          final lastModel = await _preferencesService.getLastOllamaModel();
          if (lastModel != null && _availableModels.any((m) => m.name == lastModel)) {
            _currentModel = lastModel;
            await _aiSelector.setOllamaModel(lastModel);
            debugPrint('   ‚úÖ Restaurado modelo Ollama: $lastModel');
          } else {
            debugPrint('   ‚ÑπÔ∏è Modelo Ollama no disponible, usando: $_currentModel');
          }
          break;
          
        case AIProvider.openai:
          final lastModel = await _preferencesService.getLastOpenAIModel();
          if (lastModel != null && _aiSelector.availableOpenAIModels.contains(lastModel)) {
            _aiSelector.setOpenAIModel(lastModel);
            debugPrint('   ‚úÖ Restaurado modelo OpenAI: $lastModel');
          } else {
            debugPrint('   ‚ÑπÔ∏è Usando modelo OpenAI por defecto');
          }
          break;
          
        case AIProvider.gemini:
          // Gemini usa un solo modelo, no hay que restaurar nada
          break;
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error restaurando modelo: $e');
    }
  }

  /// Alternar visibilidad del selector de modelos
  void toggleModelSelector() {
    _showModelSelector = !_showModelSelector;
    notifyListeners();
  }

  /// Ocultar selector de modelos
  void hideModelSelector() {
    if (_showModelSelector) {
      _showModelSelector = false;
      notifyListeners();
    }
  }

  /// Cambiar proveedor de IA
  Future<void> changeProvider(AIProvider provider) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando proveedor a: $provider');
      
      // Verificar disponibilidad seg√∫n el proveedor
      if (provider == AIProvider.ollama && !_aiSelector.ollamaAvailable) {
        throw Exception('Ollama no est√° disponible');
      }
      
      if (provider == AIProvider.openai && !_aiSelector.openaiAvailable) {
        throw Exception('OpenAI no est√° disponible. Configura OPENAI_API_KEY en .env');
      }
      
      await _aiSelector.setProvider(provider);
      _currentProvider = provider;
      
      // Si es Ollama y tenemos modelos, usar el modelo actual
      if (provider == AIProvider.ollama && _availableModels.isNotEmpty) {
        await _aiSelector.setOllamaModel(_currentModel);
      }
      
      // Guardar preferencia del usuario
      await _preferencesService.saveLastProvider(provider);
      
      debugPrint('   ‚úÖ Proveedor cambiado y guardado: $provider');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando proveedor: $e');
      rethrow;
    }
  }

  /// Cambiar modelo de Ollama
  Future<void> changeModel(String modelName) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando modelo a: $modelName');
      
      if (_currentProvider != AIProvider.ollama) {
        // Cambiar a Ollama autom√°ticamente si se selecciona un modelo
        await changeProvider(AIProvider.ollama);
      }
      
      await _aiSelector.setOllamaModel(modelName);
      _currentModel = modelName;
      
      // Guardar preferencia del modelo
      await _preferencesService.saveLastOllamaModel(modelName);
      
      _showModelSelector = false; // Ocultar selector despu√©s de seleccionar
      
      debugPrint('   ‚úÖ Modelo cambiado y guardado: $modelName');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo: $e');
      rethrow;
    }
  }
  
  /// Cambiar modelo de OpenAI
  Future<void> changeOpenAIModel(String modelName) async {
    try {
      debugPrint('üîÑ [ChatProvider] Cambiando modelo OpenAI a: $modelName');
      
      if (_currentProvider != AIProvider.openai) {
        // Cambiar a OpenAI autom√°ticamente si se selecciona un modelo
        await changeProvider(AIProvider.openai);
      }
      
      _aiSelector.setOpenAIModel(modelName);
      
      // Guardar preferencia del modelo
      await _preferencesService.saveLastOpenAIModel(modelName);
      
      _showModelSelector = false; // Ocultar selector despu√©s de seleccionar
      
      debugPrint('   ‚úÖ Modelo OpenAI cambiado y guardado: $modelName');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error cambiando modelo OpenAI: $e');
      rethrow;
    }
  }

  /// Refrescar modelos disponibles
  Future<void> refreshModels() async {
    try {
      debugPrint('üîÑ [ChatProvider] Refrescando modelos...');
      
      await _aiSelector.refreshOllama();
      if (_aiSelector.ollamaAvailable) {
        _availableModels = _aiSelector.availableModels;
        
        // Si el modelo actual no est√° disponible, seleccionar el primero
        if (_availableModels.isNotEmpty && 
            !_availableModels.any((m) => m.name == _currentModel)) {
          _currentModel = _availableModels.first.name;
          await _aiSelector.setOllamaModel(_currentModel);
          debugPrint('   ‚ÑπÔ∏è Modelo cambiado a: $_currentModel');
        }
      }
      
      debugPrint('   ‚úÖ Modelos refrescados');
      notifyListeners();
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error refrescando modelos: $e');
    }
  }

  /// A√±ade el mensaje de bienvenida inicial
  void _addWelcomeMessage() {
    final welcomeMessage = '''
¬°Bienvenido al chat! üëã

Aqu√≠ puedes conversar conmigo y utilizar los siguientes comandos:

**Comandos disponibles:**

- **/tryprompt** [escribe aqu√≠ tu prompt] -- Este comando te permite ejecutar un an√°lisis y mejora de tu prompt, generando como resultado un prompt mejorado en caso de que sea posible.

üí° **Nuevo:** Ahora puedes usar IA local con Ollama. Toca el bot√≥n de modelos en la parte superior para cambiar entre diferentes IAs.

¬°Empieza escribiendo tu mensaje!''';

    _messages.add(Message.bot(welcomeMessage));
    notifyListeners();
  }

  /// Env√≠a un mensaje usando el servicio actual (Gemini o Ollama)
  Future<void> sendMessage(String content) async {
    if (content.trim().isEmpty || _isProcessing) return;

    debugPrint('\nüöÄ [ChatProvider] === ENVIANDO MENSAJE ===');
    debugPrint('   üí¨ Contenido: ${content.length > 50 ? "${content.substring(0, 50)}..." : content}');
    debugPrint('   ü§ñ Proveedor actual: $_currentProvider');
    debugPrint('   üìù Modelo actual: $_currentModel');

    // Marcar que ya no es una conversaci√≥n nueva despu√©s del primer mensaje
    if (_isNewConversation) {
      _isNewConversation = false;
    }

    // Ocultar selector de modelos si est√° visible
    hideModelSelector();

    final userMessage = Message.user(content);
    _messages.add(userMessage);
    _isProcessing = true;
    notifyListeners();

    try {
      String botResponse;
      
      if (_currentProvider == AIProvider.ollama && _aiSelector.ollamaAvailable) {
        debugPrint('   üü™ Usando Ollama...');
        // Usar Ollama
        botResponse = await _sendToOllama(content);
      } else {
        debugPrint('   üü¶ Usando Gemini...');
        // Usar Gemini (comportamiento original)
        final response = await _sendMessageUseCase.execute(content);
        botResponse = response.content;
      }
      
      _messages.add(Message.bot(botResponse));
      debugPrint('‚úÖ [ChatProvider] Mensaje procesado exitosamente');
      debugPrint('üü¢ [ChatProvider] === ENV√çO EXITOSO ===\n');
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error procesando mensaje: $e');
      debugPrint('üî¥ [ChatProvider] === ENV√çO FALLIDO ===\n');
      
      String errorMessage = '‚ùå Error inesperado: ${e.toString()}';
      
      // Si falla Ollama, ofrecer cambiar a Gemini
      if (_currentProvider == AIProvider.ollama) {
        errorMessage += '\n\nüí° ¬øQuieres probar con Gemini? Toca el selector de modelos arriba.';
      }
      
      _messages.add(Message.bot(errorMessage));
    } finally {
      _isProcessing = false;
      _updateQuickResponses();
      notifyListeners();

      // üîπ Guarda autom√°ticamente cada vez que cambia la conversaci√≥n
      await _autoSaveConversation();
    }
  }

  /// Enviar mensaje a Ollama
  Future<String> _sendToOllama(String content) async {
    try {
      debugPrint('   üì§ [ChatProvider] Preparando mensaje para Ollama...');
      debugPrint('   üéØ Modelo: $_currentModel');
      
      // Si es un comando, usar el command processor existente con Gemini
      if (content.startsWith('/')) {
        debugPrint('   üî∏ Detectado comando, usando Gemini para procesarlo');
        final response = await _sendMessageUseCase.execute(content);
        return response.content;
      }

      // Para mensajes normales, usar Ollama directamente a trav√©s del selector
      debugPrint('   üí¨ Enviando mensaje normal a Ollama');
      
      // Usar el selector de AI que ya maneja la verificaci√≥n y el env√≠o
      final response = await _aiSelector.sendMessage(
        content,
        history: _messages.where((m) => !m.content.contains('¬°Bienvenido al chat!')).toList(),
      );
      
      debugPrint('   ‚úÖ Respuesta recibida de Ollama (${response.length} caracteres)');
      return response;
    } catch (e) {
      debugPrint('   ‚ùå Error con Ollama: $e');
      throw Exception('Error con Ollama: $e');
    }
  }

  /// Convertir historial a formato de chat para Ollama
  List<ChatMessage> _convertHistoryToChatMessages(List<Message> history, String newMessage) {
    final messages = <ChatMessage>[
      ChatMessage(
        role: 'system',
        content: 'Eres un asistente de IA √∫til y educativo especializado en ense√±ar sobre inteligencia artificial y prompting. Responde de manera clara, educativa y pr√°ctica.',
      ),
    ];

    // Agregar historial (√∫ltimos 10 mensajes para no sobrecargar)
    final recentHistory = history.length > 10 ? history.sublist(history.length - 10) : history;

    for (final msg in recentHistory) {
      // Saltar mensaje de bienvenida
      if (msg.content.contains('¬°Bienvenido al chat!')) continue;
      
      messages.add(ChatMessage(
        role: msg.isUser ? 'user' : 'assistant',
        content: msg.content,
      ));
    }

    return messages;
  }

  void _updateQuickResponses() {
    _quickResponses = QuickResponseProvider.getContextualResponses(_messages);
  }

  /// Guarda autom√°ticamente la conversaci√≥n actual
  Future<void> _autoSaveConversation() async {
    if (_messages.isEmpty) return;
    try {
      await ConversationRepository.saveConversation(_messages);
      if (kDebugMode) {
        debugPrint("üíæ [ChatProvider] Conversaci√≥n guardada autom√°ticamente (${_messages.length} mensajes)");
      }
    } catch (e) {
      debugPrint('‚ùå [ChatProvider] Error al guardar conversaci√≥n: $e');
    }
  }

  /// Limpia el chat (opcionalmente guardando antes)
  Future<void> clearMessages({bool saveBeforeClear = true}) async {
    debugPrint('üóëÔ∏è [ChatProvider] Limpiando mensajes...');
    
    if (saveBeforeClear && _messages.isNotEmpty) {
      await ConversationRepository.saveConversation(_messages);
    }
    _messages.clear();
    _isNewConversation = true;
    
    // A√±adir mensaje de bienvenida al limpiar
    _addWelcomeMessage();
    notifyListeners();
    
    debugPrint('   ‚úÖ Mensajes limpiados');
  }

  /// Carga una conversaci√≥n desde un archivo
  Future<void> loadConversation(File file) async {
    debugPrint('üìÇ [ChatProvider] Cargando conversaci√≥n desde archivo...');
    
    final loadedMessages = await ConversationRepository.loadConversation(file);
    _messages
      ..clear()
      ..addAll(loadedMessages);
    
    // Marcar como conversaci√≥n existente (no mostrar mensaje de bienvenida)
    _isNewConversation = false;
    
    _updateQuickResponses();
    notifyListeners();
    
    debugPrint('   ‚úÖ Conversaci√≥n cargada (${_messages.length} mensajes)');
  }

  @override
  void dispose() {
    debugPrint('üî¥ [ChatProvider] Disposing...');
    _aiSelector.dispose();
    super.dispose();
  }
}